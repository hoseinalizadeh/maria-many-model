{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Pipeline\n",
    "---\n",
    "\n",
    "In this notebook we create a pipeline to forecast sales with the models we trained in the last step. The forecasting pipeline we'll set up is similar to the training pipeline in the last step. For more details on the steps and functions refer to that notebook.\n",
    "\n",
    "We will set up the Pipeline for forecasting given the desired forecasting horizon. We utitlize the ParallelRunStep to parallelize the process. For more information about the Data and Models refer to the Data Preparation and Training Notebooks.\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "At this point, you should have already:\n",
    "\n",
    "1. Created your AML Workspace\n",
    "2. Run 00_Environment_Setup.ipynb to configure the enviroment\n",
    "3. Run 01_Training_Pipeline.ipynb to train the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to workspace and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# set up datastores\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "print('Workspace Name: ' + ws.name, \n",
    "      'Azure Region: ' + ws.location, \n",
    "      'Subscription Id: ' + ws.subscription_id, \n",
    "      'Resource Group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'forecasting_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 3.0 Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "small_dataset = Dataset.get_by_name(ws, name='oj_data_small')\n",
    "small_dataset_input = small_dataset.as_named_input('forecast_10_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Create ParallelRunStep for the forecasting pipeline\n",
    "\n",
    "### 4.1 Configure environment for ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "forecast_env = Environment(name=\"many_models_environment\")\n",
    "forecast_conda_deps = CondaDependencies.create(pip_packages=['sklearn'])\n",
    "forecast_env.python.conda_dependencies = forecast_conda_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Choose a compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "compute = AmlCompute(ws, \"cpucluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Set up ParallelRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunConfig \n",
    "\n",
    "process_count_per_node = 8\n",
    "node_count = 5\n",
    "timeout = 500\n",
    "\n",
    "tags = {}\n",
    "tags['node_count'] = node_count\n",
    "tags['process_count_per_node'] = process_count_per_node\n",
    "tags['timeout'] = timeout\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='forecast.py',\n",
    "    mini_batch_size='1',\n",
    "    run_invocation_timeout=timeout, \n",
    "    error_threshold=10,\n",
    "    output_action='append_row', \n",
    "    environment=forecast_env, \n",
    "    process_count_per_node=process_count_per_node, \n",
    "    compute_target=compute, \n",
    "    node_count=node_count\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Set up ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep \n",
    "\n",
    "output_dir = PipelineData(name='output_dir', datastore=dstore)\n",
    "\n",
    "parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-forecasting\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[small_dataset_input],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False,\n",
    "    arguments=['--forecast_horizon', 8,\n",
    "              '--starting_date', '1992-10-01',\n",
    "              '--target_column', 'Quantity',\n",
    "              '--timestamp_column', 'WeekStarting',\n",
    "              '--model_type', 'lr',\n",
    "              '--date_freq', 'W-THU']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Create PythonScriptStep to copy predictions at the end of the pipeline\n",
    "\n",
    "### 5.1 Create a data reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "output_dstore = Datastore.register_azure_blob_container(\n",
    "    workspace=ws, \n",
    "    datastore_name=\"predictions\",\n",
    "    container_name=\"predictions\",\n",
    "    account_name=dstore.account_name,\n",
    "    account_key=dstore.account_key,\n",
    "    create_if_not_exists=True\n",
    ")\n",
    "\n",
    "output_dref = DataReference(output_dstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create PythonScriptStep to copy predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "upload_predictions_step = PythonScriptStep(\n",
    "    name=\"copy_predictions\",\n",
    "    script_name=\"copy_predictions.py\",\n",
    "    compute_target=compute,\n",
    "    source_directory='./scripts',\n",
    "    inputs=[output_dref, output_dir],\n",
    "    allow_reuse=False,\n",
    "    arguments=['--parallel_run_step_output', output_dir,\n",
    "              '--output_dir', output_dref]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallel_run_step, upload_predictions_step])\n",
    "run = experiment.submit(pipeline, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_predictions(run, target_dir=None):\n",
    "    stitch_run = run.find_step_run(\"many-models-forecasting\")[0]\n",
    "    \n",
    "    port_data = stitch_run.get_output_data(\"output_dir\")\n",
    "    print(port_data)\n",
    "    port_data.download(target_dir, show_progress=True)\n",
    "    step_hash = os.listdir(os.path.join(target_dir, 'azureml'))[0]\n",
    "    return  os.path.join(target_dir, 'azureml', step_hash, 'output_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(output_path)\n",
    "fileNames = []\n",
    "\n",
    "col = []\n",
    "for f in files[1:]: \n",
    "    fileNames.append(pd.read_csv(output_path + '/' + f))\n",
    "#concat df and set index to week starting \n",
    "df = pd.concat(fileNames, ignore_index=True)\n",
    "df.WeekStarting = pd.to_datetime(df.WeekStarting)\n",
    "df.WeekStarting = [d.date() for d in df.WeekStarting]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.violinplot(x=df['Brand'], y=df['Predictions'], data=df)\n",
    "fig.set_title('Predictions by Brand')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

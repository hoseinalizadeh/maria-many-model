{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "We do this using a 'pipeline first mentality' i.e. we want to have a production pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Experiment\n",
    "from azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE, CondaDependencies\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.pipeline.core import Pipeline, PipelineDataset, PipelineParameter, PipelineData, TrainingOutput, Schedule\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.train.automl import AutoMLStep, AutoMLConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace, experiment and compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "dstor = ws.datastores[\"datalake\"]\n",
    "ds_name = \"AirliftDataset-Train\"\n",
    "experiment = Experiment(ws, \"airlift\")\n",
    "aml_compute_target = AmlCompute(ws, \"onenode-cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "conda_run_config.target = aml_compute_target\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "cd = CondaDependencies.create(pip_packages=[\"azureml-sdk[automl]\", \"scikit-learn\", \"azureml-explain-model\"], conda_packages=[\"numpy\"])\n",
    "conda_run_config.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data Step\n",
    "\n",
    "We need to bring the data into a common X, y format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = PipelineParameter(\"column_name\", default_value=\"BOUGHT_CATEGORY_10\")\n",
    "output_split_train_x = PipelineData(\"output_split_train_x\", datastore=dstor)\n",
    "output_split_train_y = PipelineData(\"output_split_train_y\", datastore=dstor)\n",
    "output_split_test_x = PipelineData(\"output_split_test_x\", datastore=dstor)\n",
    "output_split_test_y = PipelineData(\"output_split_test_y\", datastore=dstor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_split_step = PythonScriptStep(script_name=\"train_test_split.py\", \n",
    "                                         allow_reuse=True,\n",
    "                                         name=\"test_train_split_step\",\n",
    "                                         arguments=[\"--target_column\", target_name,\n",
    "                                                       \"--input_dataset\", ds_name,\n",
    "                                                       \"--output_split_train_x\", output_split_train_x,\n",
    "                                                       \"--output_split_train_y\", output_split_train_y,\n",
    "                                                       \"--output_split_test_x\", output_split_test_x,\n",
    "                                                       \"--output_split_test_y\", output_split_test_y],\n",
    "                                         outputs=[output_split_train_x, output_split_train_y, output_split_test_x, output_split_test_y], \n",
    "                                         compute_target=aml_compute_target, \n",
    "                                         runconfig=conda_run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable of X train split: $AZUREML_DATAREFERENCE_output_split_train_x\n",
      "Environment variable of y train split: $AZUREML_DATAREFERENCE_output_split_train_y\n"
     ]
    }
   ],
   "source": [
    "print(\"Environment variable of X train split: \" + output_split_train_x.__str__())\n",
    "print(\"Environment variable of y train split: \" + output_split_train_y.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_data = PipelineData(name=\"automl_metrics\",\n",
    "                            datastore=dstor, \n",
    "                            pipeline_output_name=\"metrics_output\",\n",
    "                            training_output=TrainingOutput(type=\"Metrics\"))\n",
    "\n",
    "model_data = PipelineData(name=\"automl_model\", \n",
    "                          datastore=dstor,\n",
    "                          pipeline_output_name=\"best_model_output\",\n",
    "                          training_output=TrainingOutput(type=\"Model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task = \"classification\",\n",
    "                             iterations = 1,\n",
    "                             iteration_timeout_minutes = 5, \n",
    "                             max_cores_per_iteration = 4,\n",
    "                             max_concurrent_iterations = 1,\n",
    "                             primary_metric = \"accuracy\",\n",
    "                             path=\".\",\n",
    "                             data_script = \"get_data.py\",\n",
    "                             run_configuration = conda_run_config,\n",
    "                             model_explainability = True,\n",
    "                             n_cross_validations = 2,\n",
    "                             preprocess = True,\n",
    "                             compute_target=aml_compute_target)\n",
    "\n",
    "automl_step = AutoMLStep(name=\"build_model\", \n",
    "                         allow_reuse=True,\n",
    "                         automl_config=automl_config,\n",
    "                         inputs=[output_split_train_x, output_split_train_y],\n",
    "                         outputs=[metrics_data, model_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_model_step = PythonScriptStep(script_name=\"register_model.py\",\n",
    "                                       name=\"register_model\",\n",
    "                                       allow_reuse=True,\n",
    "                                       arguments=[\"--model_id\", target_name, \"--model_path\", model_data, \"--ds_name\", ds_name],\n",
    "                                       inputs=[model_data],\n",
    "                                       compute_target=aml_compute_target,\n",
    "                                       runconfig=conda_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'auto_prepare_environment' is deprecated and unused. It will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In get_data\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(ws, steps=[test_train_split_step, automl_step, register_model_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step test_train_split_step [b3df1883][eb7dcc08-deb1-4e8f-9607-984c436dba97], (This step will run and generate new outputs)\n",
      "Created step build_model [45479c99][d407485f-493a-419a-a16f-f0a60f704b9e], (This step will run and generate new outputs)\n",
      "Created step register_model [81524e61][2a92a65a-81bf-48c9-9e84-8686046d82ba], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: c1668258-e30e-4afd-85f7-511c36586ad4\n"
     ]
    }
   ],
   "source": [
    "pipeline_run = experiment.submit(pipeline, pipeline_params={\"column_name\": \"BOUGHT_CATEGORY_3\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule the pipeline on a blob change "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prop-Training-Pipeline\n",
      "Created step test_train_split_step [1edd7555][57403a31-d904-4ef0-8a98-483a4d6775d7], (This step will run and generate new outputs)\n",
      "Created step build_model [5d6ffcd7][6fb8e31b-bb9f-4f7a-bb39-477dd086db52], (This step will run and generate new outputs)\n",
      "Created step register_model [6d711caa][92ceb30d-c92c-4c1e-874b-18d855783b3b], (This step will run and generate new outputs)\n",
      "Newly published pipeline id: 84458849-c3c2-4a85-94fc-2e4f0c1519ca\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = \"Prop-Training-Pipeline\"\n",
    "print(pipeline_name)\n",
    "\n",
    "published_pipeline = pipeline.publish(\n",
    "    name=pipeline_name, \n",
    "    description=pipeline_name)\n",
    "\n",
    "print(\"Newly published pipeline id: {}\".format(published_pipeline.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOUGHT_CATEGORY_1',\n",
       " 'BOUGHT_CATEGORY_2',\n",
       " 'BOUGHT_CATEGORY_3',\n",
       " 'BOUGHT_CATEGORY_4',\n",
       " 'BOUGHT_CATEGORY_5',\n",
       " 'BOUGHT_CATEGORY_6',\n",
       " 'BOUGHT_CATEGORY_7',\n",
       " 'BOUGHT_CATEGORY_8',\n",
       " 'BOUGHT_CATEGORY_9',\n",
       " 'BOUGHT_CATEGORY_10']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = [\"BOUGHT_CATEGORY_\" + str(i) for i in range(1,11)]\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provisioning status: Completed\n",
      "Created schedule with id: 6c42b575-1fb0-4362-83bc-dbf0cf674f08\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: 217f981c-c1cf-4cca-9d26-f17fc44394e4\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: 49d98c48-3fee-4f7a-9076-b8c85cad72f6\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: 7e8b9899-1c3a-42a4-b2e3-183ef81466ba\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: f4692732-a492-42a5-8bc5-40f74d959d6b\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: 807b3049-3f86-4a2d-908e-41784ac4ff87\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: 08965120-43e1-4ed5-9232-e293d94c454c\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: 4ac56f40-391c-4d76-948d-7fba02f791f5\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: 0b26f2b7-59e1-47c1-835c-325df14341f3\n",
      "Provisioning status: Completed\n",
      "Created schedule with id: 9bb95f38-f603-4591-8342-44b7f589b45e\n"
     ]
    }
   ],
   "source": [
    "for m in model_list:\n",
    "    schedule = Schedule.create(workspace=ws, name=m,\n",
    "                               pipeline_parameters={\"column_name\": m},\n",
    "                               pipeline_id=published_pipeline.id, \n",
    "                               experiment_name=m, # this creates parallelism on an AML cluster\n",
    "                               datastore=dstor,\n",
    "                               wait_for_provisioning=True,\n",
    "                               description=m,\n",
    "                               polling_interval=1440,\n",
    "                               path_on_datastore=\"airlift/data-latest.csv\")\n",
    "\n",
    "    print(\"Created schedule with id: {}\".format(schedule.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disable schedules as this is a demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in Schedule.list(ws):\n",
    "    s.disable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script creates a pipeline with a ParallelRunStep to score all the models and output the predictions to blob storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.core.model import Model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Workspace, Datastore, Experiment and Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the Training Pipeline notebook, we need to call the Workspace and set up an Experiment. We also want to create variables for the datastore and compute cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "ws.get_details()\n",
    "\n",
    "# define the compute cluster and the data store\n",
    "compute = AmlCompute(ws, 'cpu-cluster')\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "# set up the experiment\n",
    "experiment = Experiment(ws, 'scoring-pipeline-AP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the models\n",
    "from azureml.core.model import Model \n",
    "\n",
    "model1 = Model(ws, 'arima_Store5_tropicana')\n",
    "model2 = Model(ws, 'arima_Store2_dominicks')\n",
    "model3 = Model(ws, 'arima_Store8_minute.maid')\n",
    "\n",
    "model_list = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.core.model.Model"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "# dataset = Dataset.get_by_name(ws, name='Store2_dominicks')\n",
    "\n",
    "dataset1 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store2_dominicks.csv'))\n",
    "dataset2 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store5_tropicana.csv'))\n",
    "dataset3 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store8_minute.maid.csv'))\n",
    "\n",
    "output_dir = PipelineData(name=\"3_models\", \n",
    "                          datastore=dstore, \n",
    "                          output_path_on_compute=\"3models/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the ParallelRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parallel run config\n",
    "workercount = 3\n",
    "nodecount = 1\n",
    "timeout = 3000\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'summary_only', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname = 'store'\n",
    "output_dir = PipelineData(name = 'scoringOutput', \n",
    "                         datastore = dstore, \n",
    "                         output_path_on_compute = 'scoringOutput/')\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-scoring\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[dataset1.as_named_input(datasetname), dataset2.as_named_input(datasetname), dataset3.as_named_input(datasetname)], # must have at least one element.... \n",
    "    output=output_dir,\n",
    "    models= model_list, # this is just for logging\n",
    "    arguments=['--n_predictions', 6],\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step many-models-scoring [4e56035e][3ccd7e72-45fd-42dd-a81f-a099067e9b41], (This step will run and generate new outputs)\n",
      "Using data reference store5_0 for StepId [512dd7d7][e79bdf2c-906d-4a00-a053-8d1dc436f342], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun 18c39a34-306b-4ab2-b756-e215c6657122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Converting non-string tag to string: (nodes: 1)\n",
      "WARNING - Converting non-string tag to string: (workers-per-node: 3)\n",
      "WARNING - Converting non-string tag to string: (timeout: 3000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/scoring-pipeline-AP/runs/18c39a34-306b-4ab2-b756-e215c6657122?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(ws, steps=[parallelrun_step])\n",
    "\n",
    "run = experiment.submit(pipeline, tags=tags1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the Output from the Pipeline\n",
    "Put the predicitons back into blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_run = next(run.get_children())\n",
    "prediction_output = prediction_run.get_output_data(\"3models\")\n",
    "prediction_output\n",
    "\n",
    "prediction_output.download(local_path=\"training_results\")\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"training_results\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "            \n",
    "df = pd.read_csv(result_file, delimiter=\" \", header=None) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/score.py\n",
    "from azureml.core.run import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "# import datetime\n",
    "from entry_script_helper import EntryScriptHelper\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "\n",
    "\n",
    "thisrun = Run.get_context()\n",
    "#childrun=thisrun\n",
    "\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--n_test_set\", type=int, help=\"input number of predictions\")\n",
    "parser.add_argument(\"--timestamp_column\", type=str, help=\"model name\")\n",
    "#parser.add_argument(\"--start_date\", type=str, help=\"date to start predictions\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "# args = parser.parse_args()\n",
    "\n",
    "print(\"Argument 1(n_test_set): %s\" % args.n_test_set)\n",
    "print(\"Argument 2(timestamp_column): %s\" % args.timestamp_column)\n",
    "\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")    \n",
    "    return\n",
    "\n",
    "def run(data):\n",
    "    print(\"begin run \")\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    \n",
    "    predictions = pd.DataFrame()\n",
    "    \n",
    "    logger.info('making predictions...')\n",
    "    \n",
    "    for file in data:\n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "\n",
    "        #for w in range(0,1):\n",
    "        with thisrun.child_run(name=mname) as childrun:\n",
    "            for w in range(0,5):\n",
    "                thisrun.log(mname,str(w))\n",
    "            \n",
    "            date1=datetime.datetime.now()\n",
    "            logger.info('starting ('+file+') ' + str(date1))\n",
    "            childrun.log(mname,'starttime-'+str(date1))\n",
    "            \n",
    "            # 0. unpickle model \n",
    "            model_name = 'arima_'+str(data).split('/')[-1][:-6]\n",
    "            model_path = Model.get_model_path(model_name) \n",
    "           \n",
    "            print(model_name)\n",
    "            model = joblib.load(model_path)\n",
    "            print(\"UNPICKELED THE MODEL\")\n",
    "            \n",
    "            # 1. make preidtions \n",
    "            prediction_list, conf_int = model.predict(args.n_test_set, return_conf_int = True)\n",
    "            print(\"MADE PREDICTIONS\")\n",
    "            print(predictions)\n",
    "            \n",
    "            # 2. Score predictions with test set \n",
    "            data = pd.read_csv(file,header=0, )\n",
    "            logger.info(data.head())\n",
    "             \n",
    "            # splitting the data for test set     \n",
    "            data = data.set_index(args.timestamp_column)             \n",
    "            max_date = datetime.datetime.strptime(data.index.max(),'%Y-%m-%d')\n",
    "            split_date = max_date - timedelta(days=7*args.n_test_set)\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            #train = data[data.index <= split_date]\n",
    "            test = data[data.index > split_date]\n",
    "                \n",
    "            test['Predictions'] = prediction_list\n",
    "            print(test.head())\n",
    "            \n",
    "            # accuracy metrics \n",
    "            #accuracy_metrics = get_accuracy_metrics(test['Quantity'], test['Predictions'])\n",
    "            #print(accuracy_metrics)\n",
    "            #logger.info(accuracy_metrics)\n",
    "                        \n",
    "            metrics = []\n",
    "            mse = mean_squared_error(test['Quantity'], test['Predictions'])\n",
    "            print(mse)\n",
    "            rmse = np.sqrt(mse)\n",
    "            print(rmse)\n",
    "            mae = mean_absolute_error(test['Quantity'], test['Predictions'])\n",
    "            print(mae)\n",
    "            act, pred = np.array(test['Quantity']), np.array(test['Predictions'])\n",
    "            mape = np.mean(np.abs((act - pred)/act)*100)\n",
    "            print(mape)\n",
    "            metrics.append(mse)\n",
    "            metrics.append(rmse)\n",
    "            metrics.append(mae)\n",
    "            metrics.append(mape)\n",
    "\n",
    "            print(metrics)\n",
    "            \n",
    "            # 3. Save the output back to blob storage \n",
    "            #predictions_path = 'predictions'\n",
    "            #filename = '/arima_'+str(input_data).split('/')[-1][:-6]+'.csv'\n",
    "            \n",
    "            #test[['Quantity', 'Predictions']].to_csv(path_or_buf = predictions_path + filename, index = False)\n",
    "            \n",
    "            ws1 = childrun.experiment.workspace\n",
    "            output_path = os.path.join('./outputs/', model_name)\n",
    "            \n",
    "            test.to_csv(path_or_buf=output_path+'.csv', index = False)\n",
    "            \n",
    "            #try:\n",
    "            #    childrun.upload_file(test, output_path+'.csv')\n",
    "            #except:\n",
    "            #    logger.info('dont need to upload')\n",
    "            #logger.info('output test set, skip the outputs prefix')\n",
    "            \n",
    "            #Model.register(workspace=ws1, model_path=os.path.join('./outputs/', mname), model_name='arima_'+str(input_data).split('/')[-1][:-6], model_framework='pmdarima')\n",
    "            \n",
    "            dstore = ws1.get_default_datastore()\n",
    "            print(dstore)\n",
    "        \n",
    "            dstore.upload_files([output_path+'.csv'], target_path='oj_predictions', overwrite=False, show_progress=True)\n",
    "            \n",
    "            #you can return anything you want\n",
    "            date2=datetime.datetime.now()\n",
    "            logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "            #log some metrics\n",
    "            childrun.log(mname,'endtime-'+str(date2))\n",
    "            childrun.log(mname,'auc-1')\n",
    "        \n",
    "            predictions = predictions.append(test)\n",
    "            print(len(predictions))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

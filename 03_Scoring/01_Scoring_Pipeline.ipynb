{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we create a pipeline for Scoring the 12,000 models that we build in the Training Pipeline. We set up the Pipeline for batch scoring. We again utitlize the [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) to parallelize the process. \n",
    "\n",
    "Batch inference (or batch scoring) provides cost-effective inference, with unparalleled throughput for asynchronous applications. Batch prediction pipelines can scale to perform inference on terabytes of production data. Batch prediction is optimized for high throughput, fire-and-forget predictions for a large collection of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example runs on an Azure Machine Learning Notebook VM. We are calling models that have already been trained and registered to the Workspace. If you have already run the Environment Setup and Training Pipeline notebooks or you have an AML Notbook set up with Models registered to the Workspace you are all set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Workspace, Datastore, Experiment and Compute\n",
    "\n",
    "As we did in the Training Pipeline notebook, we need to call the Workspace and set up an Experiment. We also want to create variables for the datastore and compute cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the workspace\n",
    "\n",
    "Creat a workspace object. Workspace.from_config() reads the file config.json and loads the details into an object named ws. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "#ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "#ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the compute cluster and the data store\n",
    "compute = AmlCompute(ws, 'cpu-cluster')\n",
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Datastore containing the Orange Juice sales data\n",
    "From the Generate Data Notebook, we uploaded the csv's for each Store and Brand comination. Use the .get_default_datastore() to save the datastore we uploaded the files into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a FileDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to run on all 10,000 models \n",
    "#allfiledst = Dataset.get_by_name(ws, name='Allfiledatasets') \n",
    "#allfiledstinput = allfiledst.as_named_input('trainallmodels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell reads in 3 datasets \n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "dataset1 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store2_dominicks.csv'))\n",
    "dataset2 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store5_tropicana.csv'))\n",
    "dataset3 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store8_minute.maid.csv'))\n",
    "\n",
    "output_dir = PipelineData(name=\"3_models\", \n",
    "                          datastore=dstore, \n",
    "                          output_path_on_compute=\"3models/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Registered Models to make batch predictions\n",
    "To use the model to make batch predictions, you need an **entry script** and a list of **dependencies**:\n",
    "\n",
    "#### An entry script\n",
    "This script accepts requests, scores the requests by using the model, and returns the results.\n",
    "- __init()__ - Typically this function loads the model into a global object. This function is run only once at the start of batch processing per worker node/process. init method can make use of following environment variables (ParallelRunStep input):\n",
    "    1.\tAZUREML_BI_OUTPUT_PATH â€“ output folder path\n",
    "- __run(mini_batch)__ - The method to be parallelized. Each invocation will have one minibatch.<BR>\n",
    "__mini_batch__: Batch inference will invoke run method and pass either a list or Pandas DataFrame as an argument to the method. Each entry in min_batch will be - a filepath if input is a FileDataset, a Pandas DataFrame if input is a TabularDataset.<BR>\n",
    "__run__ method response: run() method should return a Pandas DataFrame or an array. For append_row output_action, these returned elements are appended into the common output file. For summary_only, the contents of the elements are ignored. For all output actions, each returned output element indicates one successful inference of input element in the input mini-batch.\n",
    "    User should make sure that enough data is included in inference result to map input to inference. Inference output will be written in output file and not guaranteed to be in order, user should use some key in the output to map it to input.\n",
    "    \n",
    "\n",
    "#### Dependencies\n",
    "Helper scripts or Python/Conda packages required to run the entry script or model.\n",
    "\n",
    "The deployment configuration for the compute target that hosts the deployed model. This configuration describes things like memory and CPU requirements needed to run the model.\n",
    "\n",
    "These items are encapsulated into an inference configuration and a deployment configuration. The inference configuration references the entry script and other dependencies. You define these configurations programmatically when you use the SDK to perform the deployment. You define them in JSON files when you use the CLI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Run the batch inferece pipeline\n",
    "Now that the data, models, and compute resources are set up, we can put together a pipeline for scoring. \n",
    "### Set up the environment to run the script\n",
    "Specify the conda dependencies for your script. This will allow us to install packages and configure the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDenpendencies, DEFAULT_CPU_IMAGE\n",
    "\n",
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the configuration to wrap the inference script \n",
    "In the [ParallelRunConfig](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunconfig?view=azure-ml-py), you will want to determine the number of workers and nodes appropriate for your use case. The _workercount_ is based off the number of cores on the VM. The _nodecount_ will determine the number of nodes to use. Increasing the node count should help to speed up the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig \n",
    "\n",
    "workercount = 3\n",
    "nodecount = 1\n",
    "timeout = 3000\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'summary_only', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ParallelRunStep\n",
    "This is where we will call the entry script, environment configuration, and parameters. This [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) is the main step in our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the inputs are set up for running 3 models currently. \n",
    "datasetname = 'store'\n",
    "output_dir = PipelineData(name = 'scoringOutput', \n",
    "                         datastore = dstore, \n",
    "                         output_path_on_compute = 'scoringOutput/')\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-scoring\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[dataset1.as_named_input(datasetname), dataset2.as_named_input(datasetname), dataset3.as_named_input(datasetname)],  \n",
    "    output=output_dir,\n",
    "    models= model_list, # this is just for logging\n",
    "    arguments=['--n_predictions', 6],\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and Run the Pipeline\n",
    "Create an Experiment to track the runs of the pipeline. Then, you can run the pipeline and review the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the experiment\n",
    "experiment = Experiment(ws, 'scoring-pipeline-AP')\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=[parallelrun_step])\n",
    "\n",
    "run = experiment.submit(pipeline, tags=tags1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the Output from the Pipeline\n",
    "This pipeline returns a dataframe with 8 weeks of predicitons for each Store and Brand combination. You can view the results of that dataframe from the following code. The entry script also contains code to upload the csv's individually to Blob as each process runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "prediction_run = next(run.get_children())\n",
    "prediction_output = prediction_run.get_output_data(\"3models\")\n",
    "prediction_output\n",
    "\n",
    "prediction_output.download(local_path=\"training_results\")\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"training_results\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "            \n",
    "df = pd.read_csv(result_file, delimiter=\" \", header=None) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup compute resources \n",
    "For re-occurning jobs, keeing the compute resources running may be beneficial. The compute notde will scale down to 0 when not in use. For a single run job, we want to clean up the compute resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below and run if compute resources are no longer needed \n",
    "# compute.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/score.py\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "import pickle\n",
    "import logging \n",
    "\n",
    "# Import the AzureML packages \n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# Import the helper script \n",
    "from entry_script_helper import EntryScriptHelper\n",
    "\n",
    "\n",
    "# Get the information for the current Run\n",
    "thisrun = Run.get_context()\n",
    "\n",
    "# Set the log file name\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "# Parse the arguments passed in the PipelineStep through the arguments option \n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--n_test_set\", type=int, help=\"input number of predictions\")\n",
    "parser.add_argument(\"--timestamp_column\", type=str, help=\"model name\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(\"Argument 1(n_test_set): %s\" % args.n_test_set)\n",
    "print(\"Argument 2(timestamp_column): %s\" % args.timestamp_column)\n",
    "\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")    \n",
    "    return\n",
    "\n",
    "def run(data):\n",
    "    print(\"begin run \")\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    \n",
    "    predictions = pd.DataFrame()\n",
    "    \n",
    "    logger.info('making predictions...')\n",
    "    \n",
    "    for file in data: \n",
    "    #for idx, file in enumerate(data): # add the enumerate for the 12,000 files \n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "\n",
    "        with thisrun.child_run(name=mname) as childrun:\n",
    "            for w in range(0,5):\n",
    "                thisrun.log(mname,str(w))\n",
    "            \n",
    "            date1=datetime.datetime.now()\n",
    "            logger.info('starting ('+file+') ' + str(date1))\n",
    "            childrun.log(mname,'starttime-'+str(date1))\n",
    "            \n",
    "            # 0. Unpickle Model \n",
    "            model_name = 'arima_'+str(data).split('/')[-1][:-4]  \n",
    "            print(model_name)\n",
    "            model_path = Model.get_model_path(model_name)         \n",
    "            model = joblib.load(model_path)\n",
    "            \n",
    "            # 1. Make Predictions \n",
    "            prediction_list, conf_int = model.predict(args.n_test_set, return_conf_int = True)\n",
    "            print(\"MAKING PREDICTIONS\")\n",
    "            \n",
    "             \n",
    "            # 2. Split the data for test set \n",
    "            data = pd.read_csv(file,header=0)\n",
    "            data = data.set_index(args.timestamp_column)             \n",
    "            max_date = datetime.datetime.strptime(data.index.max(),'%Y-%m-%d')\n",
    "            split_date = max_date - timedelta(days=7*args.n_test_set)\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            test = data[data.index > split_date]\n",
    "                \n",
    "            test['Predictions'] = prediction_list\n",
    "            print(test.head())\n",
    "            \n",
    "            # 3. Calculating Accuracy Metrics            \n",
    "            metrics = []\n",
    "            mse = mean_squared_error(test['Quantity'], test['Predictions'])\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(test['Quantity'], test['Predictions'])\n",
    "            act, pred = np.array(test['Quantity']), np.array(test['Predictions'])\n",
    "            mape = np.mean(np.abs((act - pred)/act)*100)\n",
    "\n",
    "            metrics.append(mse)\n",
    "            metrics.append(rmse)\n",
    "            metrics.append(mae)\n",
    "            metrics.append(mape)\n",
    "\n",
    "            print(metrics)\n",
    "            # add in a log for accuracy metrics \n",
    "            logger.info('accuracy metrics')\n",
    "            logger.info(metrics)\n",
    "            \n",
    "            # 4. Save the output back to blob storage \n",
    "            ws1 = childrun.experiment.workspace\n",
    "            output_path = os.path.join('./outputs/', model_name)\n",
    "            test.to_csv(path_or_buf=output_path+'.csv', index = False)\n",
    "            dstore = ws1.get_default_datastore()\n",
    "            dstore.upload_files([output_path+'.csv'], target_path='oj_predictions', overwrite=False, show_progress=True)\n",
    "            \n",
    "            # 5. Append the predictions to return a dataframe if desired \n",
    "            predictions = predictions.append(test)\n",
    "            \n",
    "            # 6. Return metrics for logging\n",
    "            date2=datetime.datetime.now()\n",
    "            logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "            childrun.log(mname,'endtime-'+str(date2))\n",
    "            childrun.log(mname,'auc-1')\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we create a pipeline for Scoring the 12,000 models that we build in the Training Pipeline. We set up the Pipeline for batch scoring. We again utitlize the [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) to parallelize the process. \n",
    "\n",
    "Batch inference (or batch scoring) provides cost-effective inference, with unparalleled throughput for asynchronous applications. Batch prediction pipelines can scale to perform inference on terabytes of production data. Batch prediction is optimized for high throughput, fire-and-forget predictions for a large collection of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example runs on an Azure Machine Learning Notebook VM. We are calling models that have already been trained and registered to the Workspace. If you have already run the Environment Setup and Training Pipeline notebooks you are all set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.core.model import Model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Workspace, Datastore, Experiment and Compute\n",
    "\n",
    "As we did in the Training Pipeline notebook, we need to call the Workspace and set up an Experiment. We also want to create variables for the datastore and compute cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the workspace\n",
    "\n",
    "Creat a workspace object. Workspace.from_config() reads the file config.json and loads the details into an object named ws. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "#ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "#ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the compute cluster and the data store\n",
    "compute = AmlCompute(ws, 'cpu-cluster')\n",
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Datastore containing the Orange Juice sales data\n",
    "From the Generate Data Notebook, we uploaded the csv's for each Store and Brand comination. Use the .get_default_datastore() to save the datastore we uploaded the files into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the models\n",
    "from azureml.core.model import Model \n",
    "\n",
    "model1 = Model(ws, 'arima_Store5_tropicana')\n",
    "model2 = Model(ws, 'arima_Store2_dominicks')\n",
    "model3 = Model(ws, 'arima_Store8_minute.maid')\n",
    "\n",
    "model_list = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.core.model.Model"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "# dataset = Dataset.get_by_name(ws, name='Store2_dominicks')\n",
    "\n",
    "dataset1 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store2_dominicks.csv'))\n",
    "dataset2 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store5_tropicana.csv'))\n",
    "dataset3 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store8_minute.maid.csv'))\n",
    "\n",
    "output_dir = PipelineData(name=\"3_models\", \n",
    "                          datastore=dstore, \n",
    "                          output_path_on_compute=\"3models/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the ParallelRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parallel run config\n",
    "workercount = 3\n",
    "nodecount = 1\n",
    "timeout = 3000\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'summary_only', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname = 'store'\n",
    "output_dir = PipelineData(name = 'scoringOutput', \n",
    "                         datastore = dstore, \n",
    "                         output_path_on_compute = 'scoringOutput/')\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-scoring\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[dataset1.as_named_input(datasetname), dataset2.as_named_input(datasetname), dataset3.as_named_input(datasetname)], # must have at least one element.... \n",
    "    output=output_dir,\n",
    "    models= model_list, # this is just for logging\n",
    "    arguments=['--n_predictions', 6],\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the experiment\n",
    "experiment = Experiment(ws, 'scoring-pipeline-AP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step many-models-scoring [4e56035e][3ccd7e72-45fd-42dd-a81f-a099067e9b41], (This step will run and generate new outputs)\n",
      "Using data reference store5_0 for StepId [512dd7d7][e79bdf2c-906d-4a00-a053-8d1dc436f342], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun 18c39a34-306b-4ab2-b756-e215c6657122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Converting non-string tag to string: (nodes: 1)\n",
      "WARNING - Converting non-string tag to string: (workers-per-node: 3)\n",
      "WARNING - Converting non-string tag to string: (timeout: 3000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/scoring-pipeline-AP/runs/18c39a34-306b-4ab2-b756-e215c6657122?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(ws, steps=[parallelrun_step])\n",
    "\n",
    "run = experiment.submit(pipeline, tags=tags1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the Output from the Pipeline\n",
    "Put the predicitons back into blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_run = next(run.get_children())\n",
    "prediction_output = prediction_run.get_output_data(\"3models\")\n",
    "prediction_output\n",
    "\n",
    "prediction_output.download(local_path=\"training_results\")\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(\"training_results\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "            \n",
    "df = pd.read_csv(result_file, delimiter=\" \", header=None) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/score.py\n",
    "from azureml.core.run import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "# import datetime\n",
    "from entry_script_helper import EntryScriptHelper\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "\n",
    "\n",
    "thisrun = Run.get_context()\n",
    "#childrun=thisrun\n",
    "\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--n_test_set\", type=int, help=\"input number of predictions\")\n",
    "parser.add_argument(\"--timestamp_column\", type=str, help=\"model name\")\n",
    "#parser.add_argument(\"--start_date\", type=str, help=\"date to start predictions\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "# args = parser.parse_args()\n",
    "\n",
    "print(\"Argument 1(n_test_set): %s\" % args.n_test_set)\n",
    "print(\"Argument 2(timestamp_column): %s\" % args.timestamp_column)\n",
    "\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")    \n",
    "    return\n",
    "\n",
    "def run(data):\n",
    "    print(\"begin run \")\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    \n",
    "    predictions = pd.DataFrame()\n",
    "    \n",
    "    logger.info('making predictions...')\n",
    "    \n",
    "    for file in data:\n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "\n",
    "        #for w in range(0,1):\n",
    "        with thisrun.child_run(name=mname) as childrun:\n",
    "            for w in range(0,5):\n",
    "                thisrun.log(mname,str(w))\n",
    "            \n",
    "            date1=datetime.datetime.now()\n",
    "            logger.info('starting ('+file+') ' + str(date1))\n",
    "            childrun.log(mname,'starttime-'+str(date1))\n",
    "            \n",
    "            # 0. Unpickle Model \n",
    "            model_name = 'arima_'+str(data).split('/')[-1][:-6]\n",
    "            print(model_name)\n",
    "            model_path = Model.get_model_path(model_name)         \n",
    "            model = joblib.load(model_path)\n",
    "            \n",
    "            # 1. Make Predictions \n",
    "            prediction_list, conf_int = model.predict(args.n_test_set, return_conf_int = True)\n",
    "            print(\"MAKING PREDICTIONS\")\n",
    "            \n",
    "             \n",
    "            # 2. Splitting the data for test set  \n",
    "            data = pd.read_csv(file,header=0,)\n",
    "            data = data.set_index(args.timestamp_column)             \n",
    "            max_date = datetime.datetime.strptime(data.index.max(),'%Y-%m-%d')\n",
    "            split_date = max_date - timedelta(days=7*args.n_test_set)\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            #train = data[data.index <= split_date]\n",
    "            test = data[data.index > split_date]\n",
    "                \n",
    "            test['Predictions'] = prediction_list\n",
    "            print(test.head())\n",
    "            \n",
    "            # 3. Calculating Accuracy Metrics            \n",
    "            metrics = []\n",
    "            mse = mean_squared_error(test['Quantity'], test['Predictions'])\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(test['Quantity'], test['Predictions'])\n",
    "            act, pred = np.array(test['Quantity']), np.array(test['Predictions'])\n",
    "            mape = np.mean(np.abs((act - pred)/act)*100)\n",
    "\n",
    "            metrics.append(mse)\n",
    "            metrics.append(rmse)\n",
    "            metrics.append(mae)\n",
    "            metrics.append(mape)\n",
    "\n",
    "            print(metrics)\n",
    "            \n",
    "            # 4. Save the output back to blob storage \n",
    "            ws1 = childrun.experiment.workspace\n",
    "            output_path = os.path.join('./outputs/', model_name)\n",
    "            test.to_csv(path_or_buf=output_path+'.csv', index = False)\n",
    "            dstore = ws1.get_default_datastore()\n",
    "            dstore.upload_files([output_path+'.csv'], target_path='oj_predictions', overwrite=False, show_progress=True)\n",
    "            \n",
    "            # Log metrics \n",
    "            date2=datetime.datetime.now()\n",
    "            logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "            childrun.log(mname,'endtime-'+str(date2))\n",
    "            childrun.log(mname,'auc-1')\n",
    "            \n",
    "            # 5. Append the predictions to return a dataframe if desired \n",
    "            predictions = predictions.append(test)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

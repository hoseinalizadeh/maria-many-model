{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script creates a pipeline with a ParallelRunStep to score all the models and output the predictions to blob storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.core.model import Model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Workspace, Datastore, Experiment and Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in the Training Pipeline notebook, we need to call the Workspace and set up an Experiment. We also want to create variables for the datastore and compute cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "ws.get_details()\n",
    "\n",
    "# define the compute cluster and the data store\n",
    "compute = AmlCompute(ws, 'cpu-cluster')\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "# set up the experiment\n",
    "experiment = Experiment(ws, 'scoring-pipeline-AP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the models\n",
    "from azureml.core.model import Model \n",
    "\n",
    "model1 = Model(ws, 'arima_Store5_tropicana')\n",
    "model2 = Model(ws, 'arima_Store2_dominicks')\n",
    "model3 = Model(ws, 'arima_Store8_minute.maid')\n",
    "\n",
    "model_list = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data \n",
    "dataset1 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store2_dominicks.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the ParallelRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parallel run config\n",
    "workercount = 3\n",
    "nodecount = 1\n",
    "timeout = 3000\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'summary_only', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the ParallelRunStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PipelineData(name = 'scoringOutput', \n",
    "                         datastore = dstore, \n",
    "                         output_path_on_compute = 'scoringOutput/')\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-scoring\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[dataset1.as_named_input('store5')], # must have at least one element.... \n",
    "    output=output_dir,\n",
    "    models= model_list,\n",
    "    arguments=['--n_predictions', 8],\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit and Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(ws, steps=[parallelrun_step])\n",
    "\n",
    "run = experiment.submit(pipeline, tags=tags1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scripts/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/score.py\n",
    "from azureml.core.run import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "# import datetime\n",
    "from entry_script_helper import EntryScriptHelper\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "\n",
    "thisrun = Run.get_context()\n",
    "#childrun=thisrun\n",
    "\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "print(\"Make predictions\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--n_predictions\", type=int, help=\"input number of predictions\")\n",
    "parser.add_argument(\"--model\", type=str, help=\"model name\")\n",
    "#parser.add_argument(\"--start_date\", type=str, help=\"date to start predictions\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "# args = parser.parse_args()\n",
    "\n",
    "print(\"Argument 1(n_predictions): %s\" % args.n_predictions)\n",
    "print(\"Argument 2(model): %s\" % args.model)\n",
    "\n",
    "def mape_calc(actual, predicted):\n",
    "    act, pred = np.array(actual), np.array(predicted)\n",
    "    mape = np.mean(np.abs((act - pred)/act)*100)\n",
    "    return mape\n",
    "\n",
    "def get_accuracy_metrics(actual, predicted, print_values = True):\n",
    "\n",
    "    metrics = []\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    mape = mape_calc(actual, predicted)\n",
    "    \n",
    "    metrics.append(mse)\n",
    "    metrics.append(rmse)\n",
    "    metrics.append(mae)\n",
    "    metrics.append(mape)\n",
    "    \n",
    "    if print_values == True: \n",
    "        print('Accuracy Metrics')\n",
    "        print('MSE: {}'.format(mse))\n",
    "        print('RMSE: {}'.format(rmse))\n",
    "        print('MAE: {}'.format(mae))\n",
    "        print('MAPE: {}'.format(mape))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")    \n",
    "    return\n",
    "\n",
    "def run(data):\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    resultList = []\n",
    "    logger.info('making predictions...')\n",
    "    print(\"ITERATING THROUGH MODELS\")\n",
    "    \n",
    "    for file in data:\n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "\n",
    "        #for w in range(0,1):\n",
    "        with thisrun.child_run(name=mname) as childrun:\n",
    "            for w in range(0,5):\n",
    "                thisrun.log(mname,str(w))\n",
    "            date1=datetime.datetime.now()\n",
    "            logger.info('starting ('+file+') ' + str(date1))\n",
    "            childrun.log(mname,'starttime-'+str(date1))\n",
    "            \n",
    "            # 0. unpickle model \n",
    "            model_path = Model.get_model_path(args.model)\n",
    "            print(model_path)\n",
    "            model = joblib.load(model_path)\n",
    "            print(\"UNPICKELED THE MODEL\")\n",
    "            # 1. make preidtions \n",
    "            predictions, conf_int = model.predict(args.n_predictions, return_conf_int = True)\n",
    "            print(\"MADE PREDICTIONS\")\n",
    "            print(predictions)\n",
    "            \n",
    "            # 2. Score predictions with test set \n",
    "            test = pd.read_csv(file,header=0, )\n",
    "            logger.info(data.head())\n",
    "             \n",
    "            test['Predicitons'] = predictions\n",
    "            \n",
    "            # accuracy metrics \n",
    "            accuracy_metrics = get_accuracy_metrics(test['Quantity'], test['Predictions'])\n",
    "            print(accuracy_metrics)\n",
    "            logger.info(accuracy_metrics)\n",
    "            \n",
    "            # 3. Save the output back to blob storage \n",
    "                        \n",
    "          \n",
    "                \n",
    "           \n",
    "\n",
    "            #you can return anything you want\n",
    "            date2=datetime.datetime.now()\n",
    "            logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "            #log some metrics\n",
    "            childrun.log(mname,'endtime-'+str(date2))\n",
    "            childrun.log(mname,'auc-1')\n",
    "        resultList.append(True)\n",
    "    return resultList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

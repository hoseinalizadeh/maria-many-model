{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run, Datastore, Dataset\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.core.model import Model\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the compute cluster and the data store\n",
    "compute = AmlCompute(ws, 'cpu-cluster')\n",
    "dstore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the experiment\n",
    "experiment = Experiment(ws, 'scoring-pipeline-AP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the models\n",
    "from azureml.core.model import Model \n",
    "\n",
    "model1 = Model(ws, 'arima_Store5_tropicana')\n",
    "model2 = Model(ws, 'arima_Store2_dominicks')\n",
    "model3 = Model(ws, 'arima_Store8_minute.maid')\n",
    "\n",
    "model_list = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data \n",
    "dataset1 = Dataset.File.from_files(path = (dstore, '3modelsdata/Store2_dominicks.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parallel run config\n",
    "workercount = 3\n",
    "nodecount = 1\n",
    "timeout = 3000\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'summary_only', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PipelineData(name = 'scoringOutput', \n",
    "                         datastore = dstore, \n",
    "                         output_path_on_compute = 'scoringOutput/')\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-scoring\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[dataset1.as_named_input('store5')], # must have at least one element.... \n",
    "    output=output_dir,\n",
    "    models= model_list,\n",
    "    arguments=['--n_predictions', 8],\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(ws, steps=[parallelrun_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(pipeline, tags=tags1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scripts/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/score.py\n",
    "from azureml.core.run import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "# import datetime\n",
    "from entry_script_helper import EntryScriptHelper\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "thisrun = Run.get_context()\n",
    "#childrun=thisrun\n",
    "\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "print(\"Make predictions\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--n_predictions\", type=int, help=\"input number of predictions\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "# args = parser.parse_args()\n",
    "\n",
    "print(\"Argument 1(n_predictions): %s\" % args.n_predictions)\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")\n",
    "    return\n",
    "\n",
    "def run(data):\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    resultList = []\n",
    "    logger.info('making predictions...')\n",
    "    print(\"ITERATING THROUGH MODELS\")\n",
    "    print(model)\n",
    "    print(model[0])\n",
    "    \n",
    "    for model in model:\n",
    "        u1 = uuid.uuid4()\n",
    "#         mname='arima'+str(input_data)\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "        print(model)\n",
    "        #for w in range(0,1):\n",
    "        with thisrun.child_run(name=mname) as childrun:\n",
    "            for w in range(0,5):\n",
    "                thisrun.log(mname,str(w))\n",
    "            date1=datetime.datetime.now()\n",
    "            logger.info('starting ('+model+') ' + str(date1))\n",
    "            childrun.log(mname,'starttime-'+str(date1))\n",
    "            \n",
    "            # 0. unpickle model \n",
    "            model_path = Model.get_model_path(model.name)\n",
    "            print(model.name)\n",
    "            print(model_path)\n",
    "            model = joblib.load(model_path)\n",
    "            print(\"UNPICKELED THE MODEL\")\n",
    "            # 1. make preidtions \n",
    "            predictions, conf_int = model.predict(args.n_predictions, return_conf_int = True)\n",
    "            print(\"MADE PREDICTIONS\")\n",
    "            print(predictions)\n",
    "            \n",
    "            # 2. Save the output \n",
    "            #pred_start_date = # this should be the last data of the training set.. \n",
    "            \n",
    "            # 3.train the model\n",
    "           \n",
    "            # 4. save the model\n",
    "            #logger.info(model)\n",
    "            #logger.info(mname)\n",
    "            #with open(mname, 'wb') as file:\n",
    "            #    joblib.dump(value=model, filename=os.path.join('./outputs/', mname))\n",
    "            # 5. Register the model\n",
    "            #ws1 = childrun.experiment.workspace\n",
    "            #try:\n",
    "            #    childrun.upload_file(mname, os.path.join('./outputs/', mname))\n",
    "            #except:\n",
    "            #    logger.info('dont need to upload')\n",
    "            #logger.info('register model, skip the outputs prefix')\n",
    "            #Model.register(workspace=ws1, model_path=os.path.join('./outputs/', mname), model_name='arima_'+str(input_data).split('/')[-1][:-6], model_framework='pmdarima')\n",
    "            \n",
    "            #you can return anything you want\n",
    "            date2=datetime.datetime.now()\n",
    "            logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "            #log some metrics\n",
    "            childrun.log(mname,'endtime-'+str(date2))\n",
    "            childrun.log(mname,'auc-1')\n",
    "        resultList.append(True)\n",
    "    return resultList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

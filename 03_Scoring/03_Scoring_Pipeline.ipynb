{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Scoring Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we create a pipeline for Scoring 11,973 ARIMA models that we built in the Training Pipeline of this repository. For the first run, we recommend using the subset of 10 models. This is the default set up for this notebook. We will set up the pipeline for scoring. We utitlize the [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) to parallelize the process of scoring 11,973 models. For more information about the Data and Models refer to the Data Preparation and Training Notebooks. \n",
    "\n",
    "The pipeline set up is similar to the Training Pipeline in this repository. For more details on the steps and functions refer to the Training folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example runs on an Azure Machine Learning Notebook VM. We are calling models that have **already been trained and registered** to the Workspace. We are also calling a FileDataset that has been registered to the Workspace. If you have already run the Environment Setup, Data Preparation, and Training Pipeline notebooks or you have an AML Notebook set up with Models and Data registered to the Workspace you are all set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Call the Workspace, Datastore, and Compute\n",
    "\n",
    "As we did in the Training Pipeline notebook, we need to call the Workspace. We also want to create variables for the datastore and compute cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace \n",
    "\n",
    "ws= Workspace.from_config(path='../aml_config/.azureml/ws_config.json')\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach existing compute resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "\n",
    "compute = AmlCompute(ws, 'train-many-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'scoring-pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "dstore = ws.get_default_datastore()\n",
    "score_output_dstore = Datastore(ws,'scoring_output_datastore' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Call Registered FileDataset\n",
    "In the Data Preparation notebook, we registered the orange juice data to the Workspace. You can choose to run the pipeline on the subet of 10 series or the full dataset of 11,973 series. We recommend starting with 10 series then expanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "filedst_10_models = Dataset.get_by_name(ws, name='oj_data_small')\n",
    "filedst_10_models_input = filedst_10_models.as_named_input('forecast_10_models')\n",
    "\n",
    "filedst_all_models = Dataset.get_by_name(ws, name='oj_data')\n",
    "filedst_all_models_input = filedst_all_models.as_named_input('forecast_all_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Build the scoring pipeline\n",
    "Now that the data, models, and compute resources are set up, we can put together a pipeline for scoring. \n",
    "### Set up the environment to run the script\n",
    "Specify the conda dependencies for your script. This will allow us to install packages and configure the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "score_env = Environment(name=\"many_models_environment\")\n",
    "score_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "score_env.python.conda_dependencies = score_conda_deps\n",
    "score_env.docker.enabled = True\n",
    "score_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the configuration to wrap the entry script \n",
    "In the [ParallelRunConfig](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunconfig?view=azure-ml-py), we will call the entry script, environment configuration, and parameters. You will want to determine the number of workers and nodes appropriate for your use case. We use the same settings determined in the Training Notebook. Refer to the Additional_Docs folder to create a custom entry_script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig \n",
    "\n",
    "workercount = 8\n",
    "nodecount = 5\n",
    "timeout = 500\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'append_row', \n",
    "    environment = score_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ParallelRunStep\n",
    " The [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) is the main step in our pipeline. We specify the following parameters: **input**, **output**, and **arguments**. We also set the output directory.   \n",
    "\n",
    "For the orange juice sales example, we pass four **arguments** to the entry_script. \n",
    "- **n_test_periods** is the length of the test set which is also the number of predictions you would like to make.\n",
    "- **time_column_name** is the date column from the data. \n",
    "- **output_datastore** is the datastore on blob you would like the output to be written to. \n",
    "- **overwrite_scoring** sets if the forecasts will be overwritten.\n",
    "\n",
    "*arguments* and *inputs* are the two parameters that can pass information to the entry_script.\n",
    "\n",
    "You can change between running the pipeline on a subset of models or the full data set by changing the inputs parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "datasetname = 'store'\n",
    "output_dir = PipelineData(name = 'scoringOutput', \n",
    "                         datastore = dstore)\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-scoring\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[filedst_10_models_input], \n",
    "    #inputs=[filedst_all_models_input],\n",
    "    output=output_dir,\n",
    "    models= [], \n",
    "    arguments=['--n_test_periods', 6,\n",
    "              '--timestamp_column', 'WeekStarting',\n",
    "              '--output_datastore', score_output_dstore.name,\n",
    "              '--overwrite_scoring', True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Output Step\n",
    "We create a [PythonScriptStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep) as a second step in the pipeline to retrieve the output from the ParallelRunStep and upload the results to a specificed path in Blob storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up RunConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "log_run_config = RunConfiguration(framework=\"python\")\n",
    "log_run_config.target = compute\n",
    "log_run_config.environment.docker.enabled = True\n",
    "log_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "log_cd = CondaDependencies.create(pip_packages=['azureml-pipeline-core'], conda_packages=['pandas'])\n",
    "log_run_config.environment.python.conda_dependencies = log_cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PythonScriptStep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "log_python_step = PythonScriptStep(name=\"logging\",\n",
    "                        script_name=\"log.py\",\n",
    "                        compute_target=compute,\n",
    "                        source_directory='./scripts',\n",
    "                        runconfig=log_run_config,\n",
    "                        arguments=['--parallelrunstep_name',parallelrun_step.name, \n",
    "                                   '--pipeline_output_name', output_dir.name, \n",
    "                                   '--datastore', score_output_dstore.name, \n",
    "                                   '--experiment', experiment.name, \n",
    "                                   '--overwrite_predictions', True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up step sequence  \n",
    "We set up a step sequence to ensure the ParallelRunStep executes before the PythonScriptStep. We want the ParallelRunStep to fully complete for the PythonScriptStep to collect the logs from each run and output them to the specificed directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import StepSequence\n",
    "\n",
    "scoring_steps = StepSequence(steps=[parallelrun_step, log_python_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Run the scoring pipeline\n",
    "We can use the Experiment we created to track the runs of the pipeline and review the output. With the current settings and the Standard_D13_V2 VM the pipeline takes approximately 1h 8m to run all 11,973 models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=scoring_steps)\n",
    "run = experiment.submit(pipeline, tags=tags1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Publish the pipeline\n",
    "After successfully setting up the pipeline, we publish the pipeline to the Workspace. [Published pipelines](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.publishedpipeline?view=azure-ml-py) create an endpoint to call the pipeline to run without having to open the code used to create it. It can be used to resubmit the pipline with different parameter inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'score_many_models',\n",
    "                                     description = 'score many models and log the run',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Schedule the pipeline monthly\n",
    "We can use the pipeline id to scheudle the pipeline to run at a specified interval. We schedule the scoring pipeline to run monthly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "    \n",
    "score_pipeline_id = published_pipeline.id\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Month\", interval=1, start_time=\"2020-01-01T11:00:00\")\n",
    "recurring_schedule = Schedule.create(ws, name=\"Scoring-Pipeline-Recurring-Schedule\", \n",
    "                            description=\"Schedule scoring Pipeline to run on the first day of every month starting Jan 1, 2020 at 11AM\",\n",
    "                            pipeline_id=score_pipeline_id, \n",
    "                            experiment_name=experiment.name, \n",
    "                            recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Pipeline Outputs\n",
    "The scoring pipeline returns one file with  predictions for each store along with accuracy metrics.The results are output to the scoring_output Blob container. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

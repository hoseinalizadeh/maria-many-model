{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Training Pipeline\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to create a pipeline that trains, scores, and registers 11,973 ARIMA models. We utilize the [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) to parallelize the process of training 11,973 models. For this solution accelerator we are using an orange juice sales dataset to predict the orange juice quantity for each brand and each store. For more information about the data refer to the Data Preparation Notebook.\n",
    "\n",
    "**Note:** The notebook has been configured to initially train only 10 models to speed up the testing process for you. After you have successfuly train 10 models, toggle the comments in the **Set up ParallelRunStep** section to train all 11,973 models\n",
    "\n",
    "### Prerequisites\n",
    "At this point, you should have already: \n",
    "1. Created your AML Workspace\n",
    "2. Run 00_Environment_Setup.ipynb to configure the enviroment\n",
    "3. Run 01_Data_Preparation.ipynb to register the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Set up workspace, datastore, experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "# set up workspace\n",
    "ws = Workspace.from_config(path='../.azureml/config.json')\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()\n",
    "\n",
    "# set up datastores\n",
    "dstore = ws.get_default_datastore()\n",
    "train_output_dstore = Datastore(ws, 'training_output_datastore')\n",
    "\n",
    "print('Workspace Name: ' + ws.name, \n",
    "      'Azure Region: ' + ws.location, \n",
    "      'Subscription Id: ' + ws.subscription_id, \n",
    "      'Resource Group: ' + ws.resource_group, \n",
    "      'Training Datastore Name: '+ train_output_dstore.name,\n",
    "      'Default Datastore Name: '+ dstore.name,\n",
    "      sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "An [experiment](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#experiments) is a foundational cloud resource that represents a collection of trials (individual model runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'training-pipeline')\n",
    "\n",
    "print('Experiment name: ' + experiment.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Call the registered FileDataset\n",
    "\n",
    "We use 11,973 datasets and ParallelRunStep to build 11,973 time-series ARIMA models to predict the quantity of each store brand.\n",
    "\n",
    "Each dataset represents a brand's 2 years orange juice sales data that contains 7 columns and 122 rows. \n",
    "\n",
    "You will need to register the datasets in the Workspace first. The Data Preparation notebook demonstrates how to register two datasets to the workspace. \n",
    "\n",
    "The registered 'oj_data_small' file dataset contains the first 10 csv files and 'oj_data' contains all 11,973 csv files. You can choose to pass either filedatasets_10_models_input or filedatasets_all_models_inputs in the ParallelRunStep.\n",
    "\n",
    "We recommend to **start with filedatasets_10_models** and make sure everything runs successfully, then scale up to filedatasets_all_models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "filedst_10_models = Dataset.get_by_name(ws, name='oj_data_small')\n",
    "filedst_10_models_input = filedst_10_models.as_named_input('train_10_models')\n",
    "\n",
    "filedst_all_models = Dataset.get_by_name(ws, name='oj_data')\n",
    "filedst_all_models_inputs = filedst_all_models.as_named_input('train_all_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Build the training pipeline\n",
    "Now that the dataset, workspace, and datastore are set up, we can put together a pipeline for training. \n",
    "\n",
    "### Set up environment  for ParallelRunStep\n",
    "[Environment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py) defines a collection of resources that we will need to run our pipelines. We configure a reproducible Python environment for our training script. We are using two Python libraries: sklearn and pmdarima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "train_env = Environment(name=\"many_models_environment\")\n",
    "train_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "train_env.python.conda_dependencies = train_conda_deps\n",
    "train_env.docker.enabled = True\n",
    "train_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a compute target\n",
    "\n",
    "Currently ParallelRunConfig only supports AMLCompute. You can change to a different compute cluster if one fails.\n",
    "\n",
    "This is the compute target we will pass into our ParallelRunConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "\n",
    "compute = AmlCompute(ws, \"train-many-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ParallelRunConfig\n",
    "\n",
    "[ParallelRunConfig](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_config.parallelrunconfig) is configuration for parallel run step. You will need to determine the number of workers and nodes appropriate for your use case. The workercount is based off the number of cores of the compute VM. The nodecount will determine the number of master nodes to use. In time-series ARIMA model scenario, increasing the node count will speed up the training process.\n",
    "\n",
    "\n",
    "* <b>node_count</b>: The number of compute nodes to be used for running the user script. We recommend to start with 5 and increase the node_count if the training time is taking too long.\n",
    "\n",
    "* <b>process_count_per_node</b>: Workercount - the number of processes per node. We are using a 8 cores computer cluster therefore we set it to 8.\n",
    "\n",
    "* <b>compute_target</b>: Only AmlCompute is supported. You can change to a different compute cluster if one fails.\n",
    "\n",
    "* <b>run_invocation_timeout</b>: The run() method invocation timeout in seconds. The timeout should be set to be higher than the maximum training time of one model (in seconds), by default it's 60. Since the model that takes the longest to train is about 120 seconds, we set it to be 500 which is greater than 120.\n",
    "\n",
    "We also added tags to preserve the information about our training cluster's node count, process count per node and dataset name. You can find the 'Tags' column in Azure Machine Learning Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunConfig\n",
    "\n",
    "workercount=8\n",
    "nodecount=5\n",
    "timeout=500\n",
    "\n",
    "datasetname='stores_filedatasets'\n",
    "\n",
    "tags1={}\n",
    "tags1['DatasetName']=datasetname\n",
    "tags1['Nodes']=nodecount\n",
    "tags1['WorkersPerNode']=workercount\n",
    "tags1['Timeout']=timeout\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='train.py',\n",
    "    mini_batch_size=\"1\",\n",
    "    run_invocation_timeout=timeout,\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=train_env,\n",
    "    process_count_per_node=workercount,\n",
    "    compute_target=compute,\n",
    "    node_count=nodecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ParallelRunStep\n",
    "\n",
    "This [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunstep?view=azure-ml-py) is the main step in our pipeline. First, we set up the output directory and define the Pipeline's output name. The datastore that stores the pipeline's output data is Workspace's default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "output_dir = PipelineData(name=\"training_output\", \n",
    "                          datastore=dstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 4 input arguments that the user can adjust for the forecasting use case.\n",
    "\n",
    "* <b>target_column</b>: The target column is the column name you'd like to predict on.\n",
    "* <b>n_test_periods</b>: The n test periods is the number of periods you'd like to hold off for testing/scoring.\n",
    "* <b>timestamp_column</b>: We set the timestamp column to be the index column for the ARIMA models to train on. \n",
    "* <b>stepwise_training</b>: Stepwise training can be set to 'True' or 'False'. 'False' will conduct a full grid search on each model when training hence will take longer to compelete. 'True' will perform stepwise training and the grid search will be stopped as soon as one of the thresholds are hit, and the best fit model at that time is returned. 'True' will speed up the training process dramatically.\n",
    "\n",
    "We specify the following parameters:\n",
    "\n",
    "* <b>name</b>: We set a name for our ParallelRunStep.\n",
    "\n",
    "* <b>parallel_run_config</b>: We then pass the previously defined ParallelRunConfig.\n",
    "\n",
    "* <b>inputs</b>: We are going to use the registered FileDataset that we called earlier in the Notebook. _inputs_ points to a registered file dataset in AML studio that points to a path in the blob container. The number of files in that path determines the number of models will be trained in the ParallelRunStep. \n",
    "\n",
    "* <b>output</b>: The output directory we just defined. A PipelineData object that corresponds to the output directory.\n",
    "\n",
    "* <b>models</b>: Zero or more model names already registered in the Azure Machine Learning model registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunStep\n",
    "\n",
    "parallel_run_step = ParallelRunStep(\n",
    "    name=\"many-models-training\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[filedst_10_models_input], # train 10 models\n",
    "#   inputs=[filedst_all_models_inputs], # switch to this inputs if train all 11,973 models\n",
    "    output=output_dir,\n",
    "    models=[],\n",
    "    arguments=['--target_column','Quantity', \n",
    "               '--n_test_periods',6, \n",
    "               '--timestamp_column','WeekStarting', \n",
    "               '--stepwise_training',True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the PythonScriptStep\n",
    "\n",
    "We then set up a PythonScriptStep to retrieve the output of the training pipeline (in this case, a file with trained models' logging information) and upload it to a dedicated blob path.\n",
    "\n",
    "### Set up RunConfiguration for PythonScriptStep\n",
    "\n",
    "Run configuration represents configuration for experiment runs targeting different compute targets in Azure Machine Learning. The RunConfiguration object encapsulates the information necessary to submit a training run in an experiment. Here we define azureml-pipeline-core and Pandas packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "log_run_config = RunConfiguration(framework=\"python\")\n",
    "log_run_config.target = compute\n",
    "log_run_config.environment.docker.enabled = True\n",
    "log_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "log_CondaDependencies = CondaDependencies.create(pip_packages=['azureml-pipeline-core'], conda_packages=['pandas'])\n",
    "log_run_config.environment.python.conda_dependencies = log_CondaDependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up PythonScriptStep\n",
    "\n",
    "We then set up a [PythonScriptStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep?view=azure-ml-py) to process our log file.\n",
    "\n",
    "We specify the following parameters:\n",
    "\n",
    "* <b>name</b>: We set a name for our PythonScriptStep.\n",
    "\n",
    "* <b>script_name</b>: The name of the log script.\n",
    "\n",
    "* <b>compute_target</b>: Set AML compute target. \n",
    "\n",
    "* <b>runconfig</b>: The RunConfiguration defined during the previous step.\n",
    "\n",
    "* <b>arguments</b>: The arguments you can specify based on your setup.\n",
    "\n",
    "We created 5 input arguments that the user can adjust for the forecasting use case.\n",
    "\n",
    "* <b>parallelrunstep_name</b>: The ParallelRunStep name that you defined in the ParallelRunStep.\n",
    "* <b>pipeline_output_name</b>: The ParallelRunStep output directory name. \n",
    "* <b>datastore</b>: The registered datastore name where you put your log file.\n",
    "* <b>experiment</b>: The name of the experiment that ParallelRunStep is running on. \n",
    "* <b>overwrite_logs</b>: 'True' will overwrite the existing log files. 'False' will not overwrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "log_python_script_step = PythonScriptStep(name=\"logging\",\n",
    "                        script_name=\"log.py\",\n",
    "                        compute_target=compute,\n",
    "                        source_directory='./scripts',\n",
    "                        runconfig=log_run_config,\n",
    "                        arguments=['--parallelrunstep_name', parallel_run_step.name,\n",
    "                                   '--datastore', train_output_dstore.name, \n",
    "                                   '--experiment', experiment.name, \n",
    "                                   '--overwrite_logs', True, \n",
    "                                   '--pipeline_output_name', output_dir.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the step sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a step sequence to make sure to execute ParallelRunStep and PythonScriptStep execute in the correct order. In this example, we run PythonScriptStep after ParallelRunStep since our log is retrieving ParallelRunStep's output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import StepSequence\n",
    "\n",
    "training_steps = StepSequence(steps=[parallel_run_step, log_python_script_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Run the training pipeline\n",
    "\n",
    "### Submit the pipeline to run\n",
    "\n",
    "Next we submit our pipeline to run. The whole training pipeline takes about 1h 11m using a Standard_D13_V2 VM with our current ParallelRunConfig setting. 10 file datasets estimated time is around 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=training_steps)\n",
    "run = experiment.submit(pipeline,tags=tags1)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the folowing command if you'd like to monitor the training process in jupyter notebook. It will stream logs live while training. \n",
    "\n",
    "**Note**: this command may not work for Notebook VM, however it should work on your local laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Publish the pipeline\n",
    "\n",
    "After a succesful run, we [publish the pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.publishedpipeline?view=azure-ml-py) to the Workspace.\n",
    "\n",
    "* <b>continue_on_step_failure</b>: Indicates whether to continue execution of other steps in the PipelineRun if a step fails; the default is false. If True, only steps that have no dependency on the output of the failed step will continue execution. The PythonScriptStep depends on the previous parallelRunStep hence we set it False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'train_many_models',\n",
    "                                     description = 'train many models and log the run',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Schedule the pipeline to run monthly\n",
    "\n",
    "A published pipeline represents a Pipeline to be submitted without the Python code which constructed it.\n",
    "\n",
    "In addition, a [PublishedPipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.publishedpipeline?view=azure-ml-py) can be used to resubmit a Pipeline with different PipelineParameter values and inputs. The following code block will retrieve all the published pipelines in the Workspace.\n",
    "\n",
    "We retrieve the published pipeline id and then can schedule this piepline to a desired cadence. In this case, we schedule the training pipeline to run on the first day of every month starting Jan 1, 2020 at 9AM UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "    \n",
    "training_pipeline_id = published_pipeline.id\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Month\", interval=1, start_time=\"2020-01-01T09:00:00\")\n",
    "recurring_schedule = Schedule.create(ws, name=\"training_pipeline_recurring_schedule\", \n",
    "                            description=\"Schedule Training Pipeline to run on the first day of every month starting Jan 1, 2020 at 9AM\",\n",
    "                            pipeline_id=training_pipeline_id, \n",
    "                            experiment_name=experiment.name, \n",
    "                            recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Review outputs of the training pipeline\n",
    "\n",
    "The training pipeline will train and register 11,973 models to the Workspace. You can review trained models in the Azure Machine Learning Studio under 'Models'.\n",
    "\n",
    "The pipeline will also generate a log file that contains each model's trianing information. The log file will be automatically uploaded to the training output blob container under a folder named as 'training_log' for monitoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "**1.0 Reporting**:\n",
    "Follow the steps in the [README](https://github.com/microsoft/solution-accelerator-many-models/blob/master/02_Training/README.md) to spin up a PowerBI dashboard to view the results of the training process\n",
    "\n",
    "**2.0 Forecasting**:\n",
    "Now that you have trained the modesl, you are ready to move to the [03_Scoring_Pipeline.ipynb](https://github.com/microsoft/solution-accelerator-many-models/blob/master/03_Forecasting/03_Forecasting_Pipeline.ipynb) notebook to use the models to forecast. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

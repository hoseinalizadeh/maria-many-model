{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02.1 Retrain Failed models\n",
    "\n",
    "This notebook demonstrates how to re-train failed models. It walks through how to download the log file, identify failed models' file names, clean the data, upload cleaned data back to the blob, then register the clean file dataset to the Workspace for re-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run this notebook only when models training failed and failures are logged into the log file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Set up Workspace and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "# Set up workspace\n",
    "ws= Workspace.from_config(path='../aml_config/ws_config.json')\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()\n",
    "\n",
    "# Set up datastores\n",
    "dstore = ws.get_default_datastore()\n",
    "train_output_dstore = Datastore(ws, 'training_output_datastore')\n",
    "\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, \n",
    "      'Training datastore name: '+ train_output_dstore.name,\n",
    "      'Default datastore name: '+ dstore.name,\n",
    "      sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Download the log file from the blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the log file from the blob. You can change the date to any date you'd like. Here we use today's date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Get today's date and set the log_filepath\n",
    "today_date = datetime.date.today()\n",
    "today_log_filepath = 'training_log_' + str(today_date) + '/training_log.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the log file from training output datastore to a local path called 'training_logs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "local_path = './training_logs'\n",
    "\n",
    "# Download log file\n",
    "train_output_dstore.download(target_path=local_path, prefix=today_log_filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Read the log file into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then read the log file into a pandas dataframe to identify failed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get filepath\n",
    "path = os.path.join(local_path, today_log_filepath)\n",
    "\n",
    "# Read log file\n",
    "df_log = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Identify failed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate a list that contains file names of all failed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames of failed models\n",
    "failed_list = list(df_log['FileName'].loc[df_log.Status.str.contains('Fail')])\n",
    "failed_list = [f.strip( ) + '.csv' for f in failed_list]\n",
    "\n",
    "print(failed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Download and read dirty data from the blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all the files that contain dirty data to a local path called 'dirty_data'. \n",
    "\n",
    "We use oj_sales_data_small as an example. You can change it to oj_sales_data if trained 11,973 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dstore_dir = 'oj_sales_data_small/'\n",
    "\n",
    "# Download dirty data\n",
    "for file in failed_list:\n",
    "    dstore.download(target_path = 'dirty_data', prefix = dstore_dir + str(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dirty data into dataframes. In this example we have 3 failed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dirty data\n",
    "df_Store1000_dominicks = pd.read_csv('dirty_data/oj_sales_data_small/Store1000_dominicks.csv')\n",
    "df_Store1032_dominicks = pd.read_csv('dirty_data/oj_sales_data_small/Store1032_dominicks.csv')\n",
    "df_Store1031_minute_maid = pd.read_csv('dirty_data/oj_sales_data_small/Store1031_minute.maid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Clean dirty data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data and identify where the data quality issues occur and clean them up.\n",
    "\n",
    "Here we use Store1031_minute_maid.csv as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "df_Store1031_minute_maid.ix[7, 'WeekStarting'] = '8/2/90'\n",
    "df_Store1031_minute_maid.ix[9, 'Quantity'] = 12020\n",
    "df_Store1031_minute_maid.ix[27, 'Quantity'] = 11002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Save and upload clean data to the blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the cleaned data to csv format and upload them to the default datastore, under directory 'clean_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local directory\n",
    "os.mkdir('clean_data')\n",
    "\n",
    "# Save dataframe to csv\n",
    "df_Store1031_minute_maid.to_csv('clean_data/Store1031_minute_maid.csv')\n",
    "df_Store1032_dominicks.to_csv('clean_data/Store1032_dominicks.csv')\n",
    "df_Store1031_minute_maid.to_csv('clean_data/Store1031_minute_maid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_dir = 'clean_data'\n",
    "\n",
    "# Upload clean data to the datastore\n",
    "dstore.upload(src_dir= clean_data_dir, \n",
    "              target_path= clean_data_dir,\n",
    "              overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Register the clean filedataset to the Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we register the clean_data folder as filedataset back to the workspace. You can now re-visit 02 Training Pipeline notebook. Then call the 'oj_data_clean' from Workspace as input filedataset in ParallelRunStep to retrain the failed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "clean_ds_name = 'oj_data_clean'\n",
    "path_on_datastore = dstore.path(clean_data_dir + '/')\n",
    "\n",
    "# Get files as input filedatasets from the path\n",
    "input_ds = Dataset.File.from_files(path = path_on_datastore, validate=False)\n",
    "\n",
    "# Register the filedatasets to the workspace\n",
    "registered_ds = input_ds.register(ws, clean_ds_name, create_new_version=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

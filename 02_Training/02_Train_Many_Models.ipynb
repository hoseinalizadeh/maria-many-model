{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "We do this using a 'pipeline first mentality' i.e. we want to have a production pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.74\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace, datastore, experiment and compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob manymodelssav16457539585 azureml-blobstore-77752be6-01b4-4a3e-9d42-03c9c0d6248f\n"
     ]
    }
   ],
   "source": [
    "# ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "# auth = InteractiveLoginAuthentication(force=True, tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
    "ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "\n",
    "# set up workspace\n",
    "# ws = Workspace.from_config()\n",
    "ws.get_details()\n",
    "\n",
    "# choose a compute target\n",
    "compute = AmlCompute(ws, \"train-cluster\")\n",
    "\n",
    "# choose a datastore\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "# choose a experiment\n",
    "experiment = Experiment(ws, 'automl-ojforecasting')\n",
    "print(dstore.name, dstore.datastore_type, dstore.account_name, dstore.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up run configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the run config for experiment to run targeting different compute targets in Azure Machine Learning. We use 2 python libraries, sklearn and pmdarima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['sklearn','pmdarima'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment defines a collection of resources that we will need to run our pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the registered dataset from Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 11,973 datasets and ParallelRunStep to build 11,973 time-series ARIMA models to predict the quantity of each store brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset represents a brand's 2 years orange juice sales data which contains 7 columns and 122 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to register all the datasets in the Workspace first. We uploaded our training data to a blob container, hence we set 'Datastore' as workspaceblobstore 'Relative path' as the correspondig directory in the blob when registering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileDst3Models = Dataset.get_by_name(ws, name='3modelsfiledataset')\n",
    "FileDst3ModelsInput = FileDst3Models.as_named_input('Train3Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileDst1000Models = Dataset.get_by_name(ws, name='1000modelsdataset')\n",
    "FileDst1000ModelsInput = FileDst1000Models.as_named_input('Train1000Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileDstAllModels = Dataset.get_by_name(ws, name='AllDataProd')\n",
    "FileDstAllModelsInputs = FileDstAllModels.as_named_input('TrainAllmodels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ParallelRunConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ParallelRunConfig, you will want to determine the number of workers and nodes appropriate for your use case. The workercount is based off the number of cores of the compute VM. The nodecount will determine the number of master nodes to use. In time-series ARIMA model scenario, increasing the node count will speed up the training process.\n",
    "\n",
    "The timeout should be set to be higher than the maximum training time of a model (in seconds), by default it's 60. \n",
    "\n",
    "We then set up the AML compute cluster.\n",
    "\n",
    "We also added tags to preserve the information about our training cluster's node count, process count per node and dataset name. You can find the 'Tags' column in Azure Machine Learning Studio.\n",
    "\n",
    "Then we define ParallelRunConfig(). entry_script is the name of the training script. The error threshold is set to 10. If 10 or more child runs fail, the rest of the pipeline will be automatically aborted. You can customize the error threshold based on your fault tolerance. \n",
    "\n",
    "output_action can be set to either 'append_row' or 'summary_only'. 'append_row' will aggragate all values output by run() method invocations into one unique file named 'parallel_run_step.txt' that is created in the output location. 'summary_only' means that user script is expected to store the output by itself. An output row is still expected for each successful input item processed. The system uses this output only for error threshold calculation (ignoring the actual value of the row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "workercount=8\n",
    "nodecount=5\n",
    "timeout=1000\n",
    "compute = AmlCompute(ws, \"train-many-model\")\n",
    "\n",
    "datasetname='AllStoresFileDatasets'\n",
    "\n",
    "tags1={}\n",
    "tags1['DatasetName']=datasetname\n",
    "tags1['Nodes']=nodecount\n",
    "tags1['WorkersPerNode']=workercount\n",
    "tags1['Timeout']=timeout\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='train.py',\n",
    "    mini_batch_size=\"1\",\n",
    "    run_invocation_timeout=timeout,\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    process_count_per_node=workercount,\n",
    "    compute_target=compute,\n",
    "    node_count=nodecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ParallelRunStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the output directory and define the Pipeline's output name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PipelineData(name=\"AllARIMAModels\", \n",
    "                          datastore=dstore, \n",
    "                          output_path_on_compute=\"AllARIMAModels/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added 4 arguments that you can customize based on the prediction goal. \n",
    "\n",
    "The target column is the column name you'd like to predict on. The n test periods is the number of periods you'd like to hold off for testing/scoring. We set the timestamp column to be the index column for the ARIMA models to train on. Stepwise training can be set to 'True' or 'False'. 'False' will conduct a full grid search on each model when training hence will take longer to compelete. 'True' will speed up the training process dramatically.\n",
    "\n",
    "'inputs' points to a registered file dataset in AML that points to a path in the blob container. The number of files in that path determines the number of models will be trained in the ParallelRunStep. 'output' is the output directory we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-training\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[FileDstAllModelsInputs],\n",
    "    output=output_dir,\n",
    "    models=[],\n",
    "    arguments=['--target_column','Quantity', '--n_test_periods',6, '--timestamp_column','WeekStarting', '--stepwise_training',True],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we submit our pipeline to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "\n",
    "run = experiment.submit(pipeline,tags=tags1)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the folowing command if you'd like to monitor the training process in jupyter notebook. It will stream logs live while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succesfully trained and registered 11,973 ARIMA models. The whole training pipeline takes 1h 16m 25s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/train.py\n",
    "\n",
    "from azureml.core.run import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from entry_script_helper import EntryScriptHelper\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "thisrun = Run.get_context()\n",
    "\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "print(\"Split the data into train and test\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--target_column\", type=str, help=\"input target column\")\n",
    "parser.add_argument(\"--n_test_periods\", type=int, help=\"input number of test periods\")\n",
    "parser.add_argument(\"--timestamp_column\", type=str, help=\"input timestamp column\")\n",
    "parser.add_argument(\"--stepwise_training\", type=str, help=\"input stepwise training True or False\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(\"Argument 1(n_test_periods): %s\" % args.n_test_periods)\n",
    "print(\"Argument 2(target_column): %s\" % args.target_column)\n",
    "print(\"Argument 3(timestamp_column): %s\" % args.timestamp_column)\n",
    "print(\"Argument 4(stepwise_training): %s\" % args.stepwise_training)\n",
    "\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")\n",
    "    return\n",
    "\n",
    "\n",
    "def run(input_data):\n",
    "    # 0. Set up logging\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    logger.info('processing all files')\n",
    "    resultList = []\n",
    "\n",
    "    # 1. Read in the data file\n",
    "    for idx, csv_file_path in enumerate(input_data):   \n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "        logs = []\n",
    "        date1=datetime.datetime.now()\n",
    "        logger.info('starting ('+csv_file_path+') ' + str(date1))\n",
    "        thisrun.log(mname,'starttime-'+str(date1))\n",
    "            \n",
    "        data = pd.read_csv(csv_file_path,header=0)\n",
    "        logger.info(data.head())\n",
    "\n",
    "        # 2. Split the data into train and test sets based on dates\n",
    "        data = data.set_index(args.timestamp_column)\n",
    "        max_date = datetime.datetime.strptime(data.index.max(),'%Y-%m-%d')\n",
    "        split_date = max_date - timedelta(days=7*args.n_test_periods)\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "        train = data[data.index <= split_date]\n",
    "        test = data[data.index > split_date]\n",
    "\n",
    "        # 3.Train the model\n",
    "        model = pm.auto_arima(train[args.target_column],\n",
    "                  start_p=0,\n",
    "                  start_q=0,\n",
    "                  test='adf', #default stationarity test is kpps\n",
    "                  max_p =3,\n",
    "                  max_d = 2,\n",
    "                  max_q=3,\n",
    "                  m=3, #number of observations per seasonal cycle\n",
    "                  #d=None,\n",
    "                  seasonal=True,\n",
    "                  #trend = None, # adjust this if the series have trend\n",
    "                  #start_P=0,\n",
    "                  #D=0,\n",
    "                  information_criterion = 'aic',\n",
    "                  trace=True, #prints status on the fits\n",
    "                  #error_action='ignore',\n",
    "                  stepwise = args.stepwise_training, # this increments instead of doing a grid search\n",
    "                  suppress_warnings = True,\n",
    "                  out_of_sample_size = 16\n",
    "                 )\n",
    "        model = model.fit(train[args.target_column])\n",
    "        logger.info('done training')\n",
    "\n",
    "        # 4. Save the model\n",
    "        logger.info(model)\n",
    "        logger.info(mname)\n",
    "        with open(mname, 'wb') as file:\n",
    "            joblib.dump(value=model, filename=os.path.join('./outputs/', mname))\n",
    "\n",
    "        # 5. Register the model to the workspace\n",
    "        ws1 = thisrun.experiment.workspace\n",
    "        try:\n",
    "            thisrun.upload_file(mname, os.path.join('./outputs/', mname))\n",
    "        except:\n",
    "            logger.info('dont need to upload')\n",
    "        logger.info('register model, skip the outputs prefix')\n",
    "        model_name = 'arima_'+str(input_data).split('/')[-1][:-6]\n",
    "        print('Trained '+ model_name)\n",
    "        \n",
    "        thisrun.register_model(model_path=mname, model_name=model_name, model_framework='pmdarima') \n",
    "        print('Registered '+ model_name)\n",
    "        \n",
    "        #6. Log some metrics       \n",
    "        date2=datetime.datetime.now()\n",
    "        logger.info('ending ('+str(csv_file_path)+') ' + str(date2))\n",
    "        \n",
    "        logs.append(str(csv_file_path).split('/')[-1][:-4])\n",
    "        logs.append(model_name)\n",
    "        logs.append(str(date1))\n",
    "        logs.append(str(date2))\n",
    "        logs.append(str(date2-date1))\n",
    "        logs.append(idx)\n",
    "        logs.append(len(input_data))\n",
    "        logs.append(thisrun.get_status())\n",
    "\n",
    "        thisrun.log(mname,'endtime-'+str(date2))\n",
    "        thisrun.log(mname,'auc-1')\n",
    "        \n",
    "    resultList.append(logs)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also incorporated logs to collect information about our training pipeline. Each row represent a trained model's record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our run details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>automl-ojforecasting</td><td>146e69ad-b054-4ea7-8783-d08825e5d20b</td><td>azureml.PipelineRun</td><td>Completed</td><td><a href=\"https://ml.azure.com/experiments/automl-ojforecasting/runs/146e69ad-b054-4ea7-8783-d08825e5d20b?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: automl-ojforecasting,\n",
       "Id: 146e69ad-b054-4ea7-8783-d08825e5d20b,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then download the pipeline output to a local path. In this case, the output is 1 master log file that is concatenated from all the returns of the ParallelRunStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_run = next(run.get_children())\n",
    "prediction_output = prediction_run.get_output_data(\"AllARIMAModels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Datastore</th><th>Path on Datastore</th><th>Produced By PipelineRun</th><th>Produced By StepRun</th></tr><tr><td>AllARIMAModels</td><td>workspaceblobstore</td><td>azureml/6b472277-d20c-43f5-a6dc-fca4fb44d419/AllARIMAModels</td><td><a href=\"https://ml.azure.com/experiments/automl-ojforecasting/runs/146e69ad-b054-4ea7-8783-d08825e5d20b?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\" target=\"_blank\" rel=\"noopener\">146e69ad-b054-4ea7-8783-d08825e5d20b</a></td><td><a href=\"https://ml.azure.com/experiments/automl-ojforecasting/runs/6b472277-d20c-43f5-a6dc-fca4fb44d419?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\" target=\"_blank\" rel=\"noopener\">6b472277-d20c-43f5-a6dc-fca4fb44d419</a></td></tr></table>"
      ],
      "text/plain": [
       "$AZUREML_DATAREFERENCE_AllARIMAModels"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_output.download(local_path=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"logs\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "            print (result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file='logs/azureml/6b472277-d20c-43f5-a6dc-fca4fb44d419/AllARIMAModels/parallel_run_step.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the log file and insert column names. For demonstratetion, we use 3 models example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Index</th>\n",
       "      <th>BatchSize</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Store2_dominicks'</td>\n",
       "      <td>'arima_Store2_dominicks'</td>\n",
       "      <td>'2019-12-13 21:26:27.832852'</td>\n",
       "      <td>'2019-12-13 21:26:29.456844'</td>\n",
       "      <td>'0:00:01.623992'</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'Running'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Store5_tropicana'</td>\n",
       "      <td>'arima_Store5_tropicana'</td>\n",
       "      <td>'2019-12-13 21:26:29.768970'</td>\n",
       "      <td>'2019-12-13 21:26:31.561115'</td>\n",
       "      <td>'0:00:01.792145'</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'Running'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Store8_minute.maid'</td>\n",
       "      <td>'arima_Store8_minute.maid'</td>\n",
       "      <td>'2019-12-13 21:26:31.871903'</td>\n",
       "      <td>'2019-12-13 21:26:33.095724'</td>\n",
       "      <td>'0:00:01.223821'</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>'Running'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               FileName                    ModelName  \\\n",
       "0    'Store2_dominicks'     'arima_Store2_dominicks'   \n",
       "1    'Store5_tropicana'     'arima_Store5_tropicana'   \n",
       "2  'Store8_minute.maid'   'arima_Store8_minute.maid'   \n",
       "\n",
       "                       StartTime                        EndTime  \\\n",
       "0   '2019-12-13 21:26:27.832852'   '2019-12-13 21:26:29.456844'   \n",
       "1   '2019-12-13 21:26:29.768970'   '2019-12-13 21:26:31.561115'   \n",
       "2   '2019-12-13 21:26:31.871903'   '2019-12-13 21:26:33.095724'   \n",
       "\n",
       "            Duration  Index  BatchSize      Status  \n",
       "0   '0:00:01.623992'      0          1   'Running'  \n",
       "1   '0:00:01.792145'      0          1   'Running'  \n",
       "2   '0:00:01.223821'      0          1   'Running'  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(result_file, converters={0: lambda x: x.strip(\"[\"),7: lambda x: x.strip(\"]\")}, delimiter=\",\", header=None) \n",
    "df.columns=['FileName','ModelName','StartTime','EndTime','Duration','Index','BatchSize','Status']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the log file to a dedicated path in the blob for PBI monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore.upload('logs/azureml/319e0d12-9aad-41c4-9a22-0e85ad11e331/ARIMAmodels/', target_path='traininglogs', overwrite=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add more relevant metrics to logs.\n",
    "2. Save log files to the dedicated path in the training script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "We do this using a 'pipeline first mentality' i.e. we want to have a production pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.74\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace, datastore, experiment and compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob manymodelssav16457539585 azureml-blobstore-77752be6-01b4-4a3e-9d42-03c9c0d6248f\n"
     ]
    }
   ],
   "source": [
    "# ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "# auth = InteractiveLoginAuthentication(force=True, tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
    "\n",
    "# set up workspace\n",
    "ws = Workspace.from_config()\n",
    "ws.get_details()\n",
    "\n",
    "# choose a compute target\n",
    "compute = AmlCompute(ws, \"train-cluster\")\n",
    "\n",
    "# choose a datastore\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "# choose a experiment\n",
    "experiment = Experiment(ws, 'automl-ojforecasting')\n",
    "print(dstore.name, dstore.datastore_type, dstore.account_name, dstore.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up run configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the run config for experiment to run targeting different compute targets in Azure Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['sklearn','pmdarima'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the registered dataset from Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used 12,222 datasets and ParallelRunStep to build 12,222 time-series ARIMA models to predict the quantity of each store brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiledst = Dataset.get_by_name(ws, name='Allfiledatasets') \n",
    "allfiledstinput = allfiledst.as_named_input('trainallmodels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment defines a collection of resources that we will need to run our Azure pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ParallelRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workercount=3\n",
    "nodecount=1\n",
    "timeout=3000\n",
    "\n",
    "output_dir = PipelineData(name=\"ARIMAmodels\", \n",
    "                          datastore=dstore, \n",
    "                          output_path_on_compute=\"ARIMAmodels/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname='store'\n",
    "\n",
    "tags1={}\n",
    "tags1['dataset']=datasetname\n",
    "tags1['nodes']=nodecount\n",
    "tags1['workers-per-node']=workercount\n",
    "tags1['timeout']=timeout\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='train.py',\n",
    "    mini_batch_size=\"1\",\n",
    "    run_invocation_timeout=timeout,\n",
    "    error_threshold=10,\n",
    "    output_action=\"summary_only\",\n",
    "    environment=batch_env,\n",
    "    process_count_per_node=workercount,\n",
    "    compute_target=compute,\n",
    "    node_count=nodecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ParallelRunStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added 3 arguments that users can customize based on the prediction goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-training\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[allfiledstinput],\n",
    "    output=output_dir,\n",
    "    models=[],\n",
    "    arguments=['--target_column','Quantity', '--n_test_periods',6, '--timestamp_column','WeekStarting'],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step many-models-training [789a3b04][2bd99029-0e1c-4fc0-bab8-2b1355ee355e], (This step will run and generate new outputs)\n",
      "Using data reference store_0 for StepId [6ebe16a1][298df9be-8daa-4a90-b703-52b6f42dd95a], (Consumers of this data are eligible to reuse prior runs.)Using data reference store_1 for StepId [a63cbffd][1f077270-fec6-4758-a05f-c122f677fe24], (Consumers of this data are eligible to reuse prior runs.)\n",
      "\n",
      "Using data reference store_2 for StepId [5cd5298d][21aa45ab-4869-448b-b5e1-8756388f371e], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun f9025dd3-9052-4a1c-b27d-0991b284547f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Converting non-string tag to string: (nodes: 1)\n",
      "WARNING - Converting non-string tag to string: (workers-per-node: 3)\n",
      "WARNING - Converting non-string tag to string: (timeout: 3000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/automl-ojforecasting/runs/f9025dd3-9052-4a1c-b27d-0991b284547f?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "\n",
    "run = experiment.submit(pipeline,tags=tags1)\n",
    "#RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/train.py\n",
    "\n",
    "from azureml.core.run import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from entry_script_helper import EntryScriptHelper\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "thisrun = Run.get_context()\n",
    "\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "print(\"Split the data into train and test\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--target_column\", type=str, help=\"input target column\")\n",
    "parser.add_argument(\"--n_test_periods\", type=int, help=\"input number of test periods\")\n",
    "parser.add_argument(\"--timestamp_column\", type=str, help=\"input timestamp column\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(\"Argument 1(n_test_periods): %s\" % args.n_test_periods)\n",
    "print(\"Argument 2(target_column): %s\" % args.target_column)\n",
    "print(\"Argument 3(timestamp_column): %s\" % args.timestamp_column)\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")\n",
    "    return\n",
    "\n",
    "def run(input_data):\n",
    "    # 0. Set up logging\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    resultList = []\n",
    "    logger.info('processing all files')\n",
    "\n",
    "    # 1. Read in the data file\n",
    "    for idx, csv_file_path in enumerate(input_data):\n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "        with thisrun.child_run(name=mname) as childrun:\n",
    "            for w in range(0,5):\n",
    "                thisrun.log(mname,str(w))\n",
    "            date1=datetime.datetime.now()\n",
    "            logger.info('starting ('+csv_file_path+') ' + str(date1))\n",
    "            childrun.log(mname,'starttime-'+str(date1))\n",
    "\n",
    "            data = pd.read_csv(csv_file_path,header=0)\n",
    "            logger.info(data.head())\n",
    "\n",
    "            # 2. Split the data into train and test sets based on dates\n",
    "            data = data.set_index(args.timestamp_column)\n",
    "            max_date = datetime.datetime.strptime(data.index.max(),'%Y-%m-%d')\n",
    "            split_date = max_date - timedelta(days=7*args.n_test_periods)\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            train = data[data.index <= split_date]\n",
    "            test = data[data.index > split_date]\n",
    "\n",
    "            # 3.Train the model\n",
    "            model = pm.auto_arima(train[args.target_column],\n",
    "                      start_p=0,\n",
    "                      start_q=0,\n",
    "                      test='adf', #default stationarity test is kpps\n",
    "                      max_p =3,\n",
    "                      max_d = 2,\n",
    "                      max_q=3,\n",
    "                      m=3, #number of observations per seasonal cycle\n",
    "                      #d=None,\n",
    "                      seasonal=True,\n",
    "                      #trend = None, # adjust this if the series have trend\n",
    "                      #start_P=0,\n",
    "                      #D=0,\n",
    "                      information_criterion = 'aic',\n",
    "                      trace=True, #prints status on the fits\n",
    "                      #error_action='ignore',\n",
    "                      stepwise = False, # this increments instead of doing a grid search\n",
    "                      suppress_warnings = True,\n",
    "                      out_of_sample_size = 16\n",
    "                     )\n",
    "            model = model.fit(train[args.target_column])\n",
    "            logger.info('done training')\n",
    "\n",
    "            # 4. Save the model\n",
    "            logger.info(model)\n",
    "            logger.info(mname)\n",
    "            with open(mname, 'wb') as file:\n",
    "                joblib.dump(value=model, filename=os.path.join('./outputs/', mname))\n",
    "\n",
    "            # 5. Register the model to the workspace\n",
    "            ws1 = childrun.experiment.workspace\n",
    "            try:\n",
    "                childrun.upload_file(mname, os.path.join('./outputs/', mname))\n",
    "            except:\n",
    "                logger.info('dont need to upload')\n",
    "            logger.info('register model, skip the outputs prefix')\n",
    "            Model.register(workspace=ws1, model_path=os.path.join('./outputs/', mname), model_name='arima_'+str(input_data).split('/')[-1][:-6], model_framework='pmdarima')\n",
    "            date2=datetime.datetime.now()\n",
    "            logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "            #6. Log some metrics\n",
    "            childrun.log(mname,'endtime-'+str(date2))\n",
    "            childrun.log(mname,'auc-1')\n",
    "        resultList.append(True)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Performance testing - Determine optimal parameters like node_count, process_count_per_node, workercount\n",
    "2. Understand and incorporate tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

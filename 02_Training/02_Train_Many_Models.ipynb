{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "We are creating a pipeline using ParallelRunStep to forecast orange juice sales. This notebook demonstrates how to create a pipeline that trains and registers 11,973 time-series models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.74\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "import datetime\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example runs on an Azure Machine Learning Notebook VM. If you have already run the Environment Setup and Data Preparation notebooks you are all set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace, datastore, experiment and compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob manymodelssav16457539585 azureml-blobstore-77752be6-01b4-4a3e-9d42-03c9c0d6248f\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()\n",
    "\n",
    "# choose a datastore\n",
    "dstore = ws.get_default_datastore()\n",
    "log_dstore = Datastore(ws, 'scoring_log_datastore')\n",
    "# choose a experiment\n",
    "experiment = Experiment(ws, 'automl-ojforecasting')\n",
    "print(dstore.name, dstore.datastore_type, dstore.account_name, dstore.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up run configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run configuration represents configuration for experiment runs targeting different compute targets in Azure Machine Learning. The RunConfiguration object encapsulates the information necessary to submit a training run in an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['sklearn','pmdarima'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment defines a collection of resources that we will need to run our pipelines. We configure a reproducible Python environment for machine learning experiments. We are using 2 Python libraries, sklearn and pmdarima. \n",
    "\n",
    "An Environment defines Python packages, environment variables, and Docker settings that are used in machine learning experiments, including in data preparation, training, and deployment to a web service. An Environment is managed and versioned in an Azure Machine Learning Workspace. You can update an existing environment and retrieve a version to reuse. Environments are exclusive to the workspace they are created in and can't be used across different workspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the registered dataset from Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 11,973 datasets and ParallelRunStep to build 11,973 time-series ARIMA models to predict the quantity of each store brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset represents a brand's 2 years orange juice sales data that contains 7 columns and 122 rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to register the datasets in the Workspace first. The Data Preparation notebook will walk you through this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileDst10Models = Dataset.get_by_name(ws, name='10modelsfiledataset')\n",
    "FileDst10ModelsInput = FileDst10Models.as_named_input('Train10Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileDst1000Models = Dataset.get_by_name(ws, name='1000modelsdataset')\n",
    "FileDst1000ModelsInput = FileDst1000Models.as_named_input('Train1000Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileDstAllModels = Dataset.get_by_name(ws, name='AllDataProd')\n",
    "FileDstAllModelsInputs = FileDstAllModels.as_named_input('TrainAllmodels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ParallelRunConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the models, you will need an entry script and a list of dependencies. The entry_script is a user script as a local file path that will be run in parallel on multiple nodes. If source_directly is present, use a relative path. Otherwise, use any path that's accessible on the machine.\n",
    "\n",
    "The <b>entry script</b> accepts requests, tain and register the model, and then returns the results.\n",
    "\n",
    "* <b>init()</b> - Typically this function loads the model into a global object. This function is run only once at the start of batch processing per worker node/process. init method can make use of following environment variables (ParallelRunStep input): \n",
    "\n",
    "                    AZUREML_BI_OUTPUT_PATH â€“ output folder path\n",
    "        \n",
    "* <b>run(mini_batch)</b> - The method to be parallelized. Each invocation will have one minibatch.\n",
    "mini_batch: Batch inference will invoke run method and pass either a list or Pandas DataFrame as an argument to the method. Each entry in mini_batch will be - a filepath if input is a FileDataset, a Pandas DataFrame if input is a TabularDataset.\n",
    "\n",
    "* <b>run method response</b>: run() method should return a Pandas DataFrame or an array. For append_row output_action, these returned elements are appended into the common output file. For summary_only, the contents of the elements are ignored. For all output actions, each returned output element indicates one successful inference of input element in the input mini-batch. User should make sure that enough data is included in inference result to map input to inference. Inference output will be written in output file and not guaranteed to be in order, user should use some key in the output to map it to input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ParallelRunConfig, you will want to determine the number of workers and nodes appropriate for your use case. The workercount is based off the number of cores of the compute VM. The nodecount will determine the number of master nodes to use. In time-series ARIMA model scenario, increasing the node count will speed up the training process.\n",
    "\n",
    "\n",
    "* <b>node_count</b>: The number of compute nodes to be used for running the user script. We recommend to start with 3 and increase the node_count if the training time is taking too long.\n",
    "\n",
    "* <b>process_count_per_node</b>: Workercount - the number of processes per node. We are using a 8 cores computer cluster therefore we set it to 8.\n",
    "\n",
    "* <b>compute_target</b>: Only AmlCompute is supported. You can change to a different compute cluster if one fails.\n",
    "\n",
    "* <b>run_invocation_timeout</b>: The run() method invocation timeout in seconds. The timeout should be set to be higher than the maximum training time of one model (in seconds), by default it's 60. Since the model that takes the longest to train is about 120 seconds, we set it to be 500 which is greater than 120.   \n",
    "\n",
    "* <b>entry_script</b>: The name of the training script.\n",
    "\n",
    "* <b>source_directory</b>: Paths to folders that contain all files to execute on the compute target (optional).\n",
    "\n",
    "* <b>environment</b>: The Python environment definition. You can configure it to use an existing Python environment or to set up a temporary environment for the experiment. The definition is also responsible for setting the required application dependencies (optional). \n",
    "    \n",
    "* <b>mini_batch_size</b>: The size of the mini-batch passed to a single run() call (optional). \n",
    "\n",
    "    * For FileDataset, it's the number of files with a minimum value of 1. You can combine multiple files into one mini-batch. The default value is 1. In this orange juice sales example, we're using FileDataset and set mini_batch_size to be 1 because we're iterating through a list of FileDataset as our input data.\n",
    "\n",
    "    * For TabularDataset, it's the size of data. Example values are 1024, 1024KB, 10MB, and 1GB. The recommended value is 1MB. The mini-batch from TabularDataset will never cross file boundaries. For example, if you have .csv files with various sizes, the smallest file is 100 KB and the largest is 10 MB. If you set mini_batch_size = 1MB, then files with a size smaller than 1 MB will be treated as one mini-batch. Files with a size larger than 1 MB will be split into multiple mini-batches. \n",
    "\n",
    "* <b>error_threshold</b>: The number of record failures for TabularDataset and file failures for FileDataset that should be ignored during processing. If the error count for the entire input goes above this value, the job will be stopped. The error threshold is for the entire input and not for individual mini-batches sent to the run() method. The range is [-1, int.max]. The -1 part indicates ignoring all failures during processing. You can customize the error threshold based on your fault tolerance. Here we set it to 10, meaning that if 10 or more jobs failed, the job will be stopped and canceled.\n",
    "\n",
    "* <b>output_action</b>: One of the following values indicates how the output will be organized -\n",
    "    * <b>summary_only</b>: The user script will store the output. ParallelRunStep will use the output only for the error threshold calculation. The parallel_run_step.txt will return \n",
    "    * <b>append_row</b>: For all input files, only one file will be created in the output folder to append all outputs separated by line. The file name will be parallel_run_step.txt. We set it to 'append_row' here because we collect the aggregated output file as our training log.\n",
    "    \n",
    "We also added tags to preserve the information about our training cluster's node count, process count per node and dataset name. You can find the 'Tags' column in Azure Machine Learning Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ParallelRunConfig Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "workercount=8\n",
    "nodecount=5\n",
    "timeout=500\n",
    "compute = AmlCompute(ws, \"train-max\")\n",
    "\n",
    "datasetname='AllStoresFileDatasets'\n",
    "\n",
    "tags1={}\n",
    "tags1['DatasetName']=datasetname\n",
    "tags1['Nodes']=nodecount\n",
    "tags1['WorkersPerNode']=workercount\n",
    "tags1['Timeout']=timeout\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='train.py',\n",
    "    mini_batch_size=\"1\",\n",
    "    run_invocation_timeout=timeout,\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    process_count_per_node=workercount,\n",
    "    compute_target=compute,\n",
    "    node_count=nodecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ParallelRunStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the output directory and define the Pipeline's output name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PipelineData(name=\"AllARIMAModels\", \n",
    "                          datastore=dstore, \n",
    "                          output_path_on_compute=\"AllARIMAModels/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 4 input arguments that the user can adjust for the forecasting use case.\n",
    "\n",
    "* <b>target_column</b>: The target column is the column name you'd like to predict on.\n",
    "* <b>n_test_periods</b>: The n test periods is the number of periods you'd like to hold off for testing/scoring.\n",
    "* <b>timestamp_column</b>: We set the timestamp column to be the index column for the ARIMA models to train on. \n",
    "* <b>stepwise_training</b>: Stepwise training can be set to 'True' or 'False'. 'False' will conduct a full grid search on each model when training hence will take longer to compelete. 'True' will perform stepwise training and the grid search will be stopped as soon as one of the thresholds are hit, and the best fit model at that time is returned. 'True' will speed up the training process dramatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the following parameters:\n",
    "\n",
    "* <b>name</b>: We set a name for our ParallelRunStep.\n",
    "\n",
    "* <b>parallel_run_config</b>: We then pass the previously defined ParallelRunConfig.\n",
    "\n",
    "* <b>inputs</b>: We are going to use the registered FileDataset that we called earlier in the Notebook. _inputs_ points to a registered file dataset in AML studio that points to a path in the blob container. The number of files in that path determines the number of models will be trained in the ParallelRunStep. \n",
    "\n",
    "* <b>output</b>: The output directory we just defined. A PipelineData object that corresponds to the output directory.\n",
    "\n",
    "* <b>models</b>: Zero or more model names already registered in the Azure Machine Learning model registry.\n",
    "\n",
    "* <b>allow_reuse</b>: Whether the step should reuse previous results when run with the same settings/inputs. If this parameter is False, a new run will always be generated for this step during pipeline execution. The default value is True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ParallelRunStep Documentation](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunstep?view=azure-ml-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-training\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[FileDst10ModelsInput],\n",
    "    output=output_dir,\n",
    "    models=[],\n",
    "    arguments=['--target_column','Quantity', '--n_test_periods',6, '--timestamp_column','WeekStarting', '--stepwise_training',True],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Python_Script_Step = PythonScriptStep(name=\"logs\",\n",
    "                        script_name=\"log2.py\",\n",
    "                        compute_target=compute,\n",
    "                        source_directory='./scripts',\n",
    "                        allow_reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we submit our pipeline to run. The whole training pipeline takes 1h 16m 25s using a Standard_D13_V2 VM and our current ParallelRunConfig setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step, Python_Script_Step])\n",
    "run = experiment.submit(pipeline,tags=tags1)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the folowing command if you'd like to monitor the training process in jupyter notebook. It will stream logs live while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succesfully trained and registered 11,973 ARIMA models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/train.py\n",
    "\n",
    "from azureml.core.run import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from entry_script_helper import EntryScriptHelper\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "thisrun = Run.get_context()\n",
    "\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "print(\"Split the data into train and test\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--target_column\", type=str, help=\"input target column\")\n",
    "parser.add_argument(\"--n_test_periods\", type=int, help=\"input number of test periods\")\n",
    "parser.add_argument(\"--timestamp_column\", type=str, help=\"input timestamp column\")\n",
    "parser.add_argument(\"--stepwise_training\", type=str, help=\"input stepwise training True or False\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(\"Argument 1(n_test_periods): %s\" % args.n_test_periods)\n",
    "print(\"Argument 2(target_column): %s\" % args.target_column)\n",
    "print(\"Argument 3(timestamp_column): %s\" % args.timestamp_column)\n",
    "print(\"Argument 4(stepwise_training): %s\" % args.stepwise_training)\n",
    "\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")\n",
    "    return\n",
    "\n",
    "\n",
    "def run(input_data):\n",
    "    # 0. Set up logging\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    logger.info('processing all files')\n",
    "    resultList = []\n",
    "\n",
    "    # 1. Read in the data file\n",
    "    for idx, csv_file_path in enumerate(input_data):       \n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "        logs = []\n",
    "        date1=datetime.datetime.now()\n",
    "        logger.info('starting ('+csv_file_path+') ' + str(date1))\n",
    "        thisrun.log(mname,'starttime-'+str(date1))\n",
    "            \n",
    "        data = pd.read_csv(csv_file_path,header=0)\n",
    "        logger.info(data.head())\n",
    "\n",
    "        # 2. Split the data into train and test sets based on dates\n",
    "        data = data.set_index(args.timestamp_column)\n",
    "        max_date = datetime.datetime.strptime(data.index.max(),'%Y-%m-%d')\n",
    "        split_date = max_date - timedelta(days=7*args.n_test_periods)\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "        train = data[data.index <= split_date]\n",
    "        test = data[data.index > split_date]\n",
    "\n",
    "        # 3.Train the model\n",
    "        model = pm.auto_arima(train[args.target_column],\n",
    "                  start_p=0,\n",
    "                  start_q=0,\n",
    "                  test='adf', #default stationarity test is kpps\n",
    "                  max_p =3,\n",
    "                  max_d = 2,\n",
    "                  max_q=3,\n",
    "                  m=3, #number of observations per seasonal cycle\n",
    "                  #d=None,\n",
    "                  seasonal=True,\n",
    "                  #trend = None, # adjust this if the series have trend\n",
    "                  #start_P=0,\n",
    "                  #D=0,\n",
    "                  information_criterion = 'aic',\n",
    "                  trace=True, #prints status on the fits\n",
    "                  #error_action='ignore',\n",
    "                  stepwise = args.stepwise_training, # this increments instead of doing a grid search\n",
    "                  suppress_warnings = True,\n",
    "                  out_of_sample_size = 16\n",
    "                 )\n",
    "        model = model.fit(train[args.target_column])\n",
    "        logger.info('done training')\n",
    "\n",
    "        # 4. Save the model\n",
    "        logger.info(model)\n",
    "        logger.info(mname)\n",
    "        with open(mname, 'wb') as file:\n",
    "            joblib.dump(value=model, filename=os.path.join('./outputs/', mname))\n",
    "\n",
    "        # 5. Register the model to the workspace\n",
    "        ws1 = thisrun.experiment.workspace\n",
    "        try:\n",
    "            thisrun.upload_file(mname, os.path.join('./outputs/', mname))\n",
    "        except:\n",
    "            logger.info('dont need to upload')\n",
    "        logger.info('register model, skip the outputs prefix')\n",
    "        model_name = 'arima_'+str(input_data).split('/')[-1][:-6]\n",
    "        print('Trained '+ model_name)\n",
    "        \n",
    "        thisrun.register_model(model_path=mname, model_name=model_name, model_framework='pmdarima',tags={'Store': str(csv_file_path).split('/')[-1][:-4].split('_')[0], 'Brand': str(csv_file_path).split('/')[-1][:-4].split('_')[1], 'ModelType':'ARIMA'}) \n",
    "        print('Registered '+ model_name)\n",
    "        \n",
    "        #6. Log some metrics       \n",
    "        date2=datetime.datetime.now()\n",
    "        logger.info('ending ('+str(csv_file_path)+') ' + str(date2))\n",
    "        \n",
    "        logs.append(str(csv_file_path).split('/')[-1][:-4].split('_')[0])\n",
    "        logs.append(str(csv_file_path).split('/')[-1][:-4].split('_')[1])\n",
    "        logs.append('ARIMA')\n",
    "        logs.append(str(csv_file_path).split('/')[-1][:-4])\n",
    "        logs.append(model_name)\n",
    "        logs.append(str(date1))\n",
    "        logs.append(str(date2))\n",
    "        logs.append(str(date2-date1))\n",
    "        logs.append(idx)\n",
    "        logs.append(len(input_data))\n",
    "        logs.append(thisrun.get_status())\n",
    "\n",
    "        thisrun.log(mname,'endtime-'+str(date2))\n",
    "        thisrun.log(mname,'auc-1')\n",
    "        \n",
    "    resultList.append(logs)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scripts/log2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/log2.py\n",
    "\n",
    "import pandas as pd\n",
    "from azureml.core.run import Run\n",
    "import os\n",
    "\n",
    "thisrun = Run.get_context()\n",
    "ws = thisrun.experiment.workspace\n",
    "\n",
    "prediction_run = next(thisrun.get_children())\n",
    "prediction_output = prediction_run.get_output_data(\"AllARIMAModels\")\n",
    "prediction_output.download(local_path=\"logs\")\n",
    "\n",
    "# check log file path\n",
    "for root, dirs, files in os.walk(\"logs\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "            print ('Log file path: ' + result_file)\n",
    "            \n",
    "# read the file and clean up data\n",
    "df_log = pd.read_csv(result_file, converters={0: lambda x: x.strip(\"[\"),10: lambda x: x.strip(\"]\")}, delimiter=\",\", header=None)\n",
    "df_log.columns=['Store','Brand','ModelType','FileName','ModelName','StartTime','EndTime','Duration','Index','BatchSize','Status']\n",
    "df_log['Store'] = df_log['Store'].apply(str).str.replace(\"'\", '')\n",
    "df_log['Brand'] = df_log['Brand'].apply(str).str.replace(\"'\", '')\n",
    "df_log['ModelType'] = df_log['ModelType'].apply(str).str.replace(\"'\", '')\n",
    "df_log['FileName'] = df_log['FileName'].apply(str).str.replace(\"'\", '')\n",
    "df_log['ModelName'] = df_log['ModelName'].apply(str).str.replace(\"'\", '')\n",
    "df_log['StartTime'] = df_log['StartTime'].apply(str).str.replace(\"'\", '')\n",
    "df_log['EndTime'] = df_log['EndTime'].apply(str).str.replace(\"'\", '')\n",
    "df_log['Duration'] = df_log['Duration'].apply(str).str.replace(\"'\", '')\n",
    "df_log['Status'] = df_log['Status'].apply(str).str.replace(\"'\", '')\n",
    "\n",
    "output_path = os.path.join('./logs/', 'training_logs')\n",
    "df_log.to_csv(path_or_buf=output_path + '.csv', index = False)\n",
    "print('Saved training_logs.csv')\n",
    "\n",
    "log_dstore = Datastore(ws, 'scoring_log_datastore')\n",
    "log_dstore.upload_files(['./logs/training_logs'+'.csv'], target_path='training_logs_'+str(datetime.datetime.now().date()), overwrite=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Path already exists. Skipping download for logs/azureml/62e70f7f-ef44-4bee-99e4-e05d9c78fe83/AllARIMAModels/parallel_run_step.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file path: logs/azureml/62e70f7f-ef44-4bee-99e4-e05d9c78fe83/AllARIMAModels/parallel_run_step.txt\n",
      "Saved training_logs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Brand</th>\n",
       "      <th>ModelType</th>\n",
       "      <th>FileName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Index</th>\n",
       "      <th>BatchSize</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Store1000</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1000_tropicana</td>\n",
       "      <td>arima_Store1000_tropicana</td>\n",
       "      <td>2019-12-18 22:32:35.336330</td>\n",
       "      <td>2019-12-18 22:32:40.534022</td>\n",
       "      <td>0:00:05.197692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Store1015</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1015_minute.maid</td>\n",
       "      <td>arima_Store1015_minute.maid</td>\n",
       "      <td>2019-12-18 22:32:40.857778</td>\n",
       "      <td>2019-12-18 22:32:47.672241</td>\n",
       "      <td>0:00:06.814463</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Store1036</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1036_minute.maid</td>\n",
       "      <td>arima_Store1036_minute.maid</td>\n",
       "      <td>2019-12-18 22:32:48.006304</td>\n",
       "      <td>2019-12-18 22:32:52.839974</td>\n",
       "      <td>0:00:04.833670</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Store1047</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1047_minute.maid</td>\n",
       "      <td>arima_Store1047_minute.maid</td>\n",
       "      <td>2019-12-18 22:32:53.132244</td>\n",
       "      <td>2019-12-18 22:32:57.852183</td>\n",
       "      <td>0:00:04.719939</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Store1058</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1058_tropicana</td>\n",
       "      <td>arima_Store1058_tropicana</td>\n",
       "      <td>2019-12-18 22:32:58.112553</td>\n",
       "      <td>2019-12-18 22:33:03.920171</td>\n",
       "      <td>0:00:05.807618</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Store         Brand ModelType                FileName  \\\n",
       "0  Store1000     tropicana     ARIMA     Store1000_tropicana   \n",
       "1  Store1015   minute.maid     ARIMA   Store1015_minute.maid   \n",
       "2  Store1036   minute.maid     ARIMA   Store1036_minute.maid   \n",
       "3  Store1047   minute.maid     ARIMA   Store1047_minute.maid   \n",
       "4  Store1058     tropicana     ARIMA     Store1058_tropicana   \n",
       "\n",
       "                      ModelName                    StartTime  \\\n",
       "0     arima_Store1000_tropicana   2019-12-18 22:32:35.336330   \n",
       "1   arima_Store1015_minute.maid   2019-12-18 22:32:40.857778   \n",
       "2   arima_Store1036_minute.maid   2019-12-18 22:32:48.006304   \n",
       "3   arima_Store1047_minute.maid   2019-12-18 22:32:53.132244   \n",
       "4     arima_Store1058_tropicana   2019-12-18 22:32:58.112553   \n",
       "\n",
       "                       EndTime         Duration  Index  BatchSize    Status  \n",
       "0   2019-12-18 22:32:40.534022   0:00:05.197692      0          1   Running  \n",
       "1   2019-12-18 22:32:47.672241   0:00:06.814463      0          1   Running  \n",
       "2   2019-12-18 22:32:52.839974   0:00:04.833670      0          1   Running  \n",
       "3   2019-12-18 22:32:57.852183   0:00:04.719939      0          1   Running  \n",
       "4   2019-12-18 22:33:03.920171   0:00:05.807618      0          1   Running  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.log import logs\n",
    "logs(run).cleanup_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./logs/training_logs.csv\n",
      "Uploaded ./logs/training_logs.csv, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_d347538f91db4a79b8b29fe99ce835bc"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dstore.upload_files(['./logs/training_logs'+'.csv'], target_path='training_logs_'+str(datetime.datetime.now().date()), overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/log.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/log.py\n",
    "\n",
    "import pandas as pd\n",
    "from azureml.core.run import Run\n",
    "import os\n",
    "\n",
    "class logs:\n",
    "    def __init__(self, returnlogs):\n",
    "        self.returnlogs = returnlogs\n",
    "        \n",
    "    def cleanup_log(self):\n",
    "        # download log file from datastore\n",
    "        prediction_run = next(self.returnlogs.get_children())\n",
    "        prediction_output = prediction_run.get_output_data(\"AllARIMAModels\")\n",
    "        prediction_output.download(local_path=\"logs\")\n",
    "        \n",
    "        # check log file path\n",
    "        for root, dirs, files in os.walk(\"logs\"):\n",
    "            for file in files:\n",
    "                if file.endswith('parallel_run_step.txt'):\n",
    "                    result_file = os.path.join(root,file)\n",
    "                    print ('Log file path: ' + result_file)\n",
    "                    \n",
    "        # read the file and clean up data\n",
    "        df_log = pd.read_csv(result_file, converters={0: lambda x: x.strip(\"[\"),10: lambda x: x.strip(\"]\")}, delimiter=\",\", header=None)\n",
    "        df_log.columns=['Store','Brand','ModelType','FileName','ModelName','StartTime','EndTime','Duration','Index','BatchSize','Status']\n",
    "        df_log['Store'] = df_log['Store'].apply(str).str.replace(\"'\", '')\n",
    "        df_log['Brand'] = df_log['Brand'].apply(str).str.replace(\"'\", '')\n",
    "        df_log['ModelType'] = df_log['ModelType'].apply(str).str.replace(\"'\", '')\n",
    "        df_log['FileName'] = df_log['FileName'].apply(str).str.replace(\"'\", '')\n",
    "        df_log['ModelName'] = df_log['ModelName'].apply(str).str.replace(\"'\", '')\n",
    "        df_log['StartTime'] = df_log['StartTime'].apply(str).str.replace(\"'\", '')\n",
    "        df_log['EndTime'] = df_log['EndTime'].apply(str).str.replace(\"'\", '')\n",
    "        df_log['Duration'] = df_log['Duration'].apply(str).str.replace(\"'\", '')\n",
    "        df_log['Status'] = df_log['Status'].apply(str).str.replace(\"'\", '')\n",
    "        df_log.to_csv('./logs/training_logs.csv')\n",
    "        print('Saved training_logs.csv')       \n",
    "        return df_log.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                         datastore_name='scoring_log_datastore', \n",
    "                                                         container_name='scoring-output', \n",
    "                                                         account_name=account_name,\n",
    "                                                         account_key=account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step logs [279ef49c][04128379-6c76-4c27-83f1-ed19cc140c27], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun a4ab2cc7-14c6-4aa4-85ce-418bf57f8831\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/automl-ojforecasting/runs/a4ab2cc7-14c6-4aa4-85ce-418bf57f8831?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2eb5f30c834dd6a79d17d8957a7398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/automl-ojforecasting/runs/a4ab2cc7-14c6-4aa4-85ce-418bf57f8831?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\", \"run_id\": \"a4ab2cc7-14c6-4aa4-85ce-418bf57f8831\", \"run_properties\": {\"run_id\": \"a4ab2cc7-14c6-4aa4-85ce-418bf57f8831\", \"created_utc\": \"2019-12-18T18:44:02.662252Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": null, \"runType\": \"HTTP\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2019-12-18T18:45:53.968833Z\", \"status\": \"Failed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://manymodelssav16457539585.blob.core.windows.net/azureml/ExperimentRun/dcid.a4ab2cc7-14c6-4aa4-85ce-418bf57f8831/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=EGB9co9RTVNCBA40%2BCnfM18zLnkCbH%2BPrjDCRhtWOsU%3D&st=2019-12-18T18%3A36%3A15Z&se=2019-12-19T02%3A46%3A15Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://manymodelssav16457539585.blob.core.windows.net/azureml/ExperimentRun/dcid.a4ab2cc7-14c6-4aa4-85ce-418bf57f8831/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=6l3BYdvUaEAb8fnSwDFqwBO5T9YNjWQ3eChyNAXUU1I%3D&st=2019-12-18T18%3A36%3A15Z&se=2019-12-19T02%3A46%3A15Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://manymodelssav16457539585.blob.core.windows.net/azureml/ExperimentRun/dcid.a4ab2cc7-14c6-4aa4-85ce-418bf57f8831/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=aAEKK3mNFAEpCMGsgxjyBxq%2FDS3PtjtLOt%2FqJfPeiRQ%3D&st=2019-12-18T18%3A36%3A15Z&se=2019-12-19T02%3A46%3A15Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:01:51\"}, \"child_runs\": [{\"run_id\": \"fe368603-209a-4056-acf1-4854a1a1e901\", \"name\": \"logs\", \"status\": \"Failed\", \"start_time\": \"2019-12-18T18:44:50.384413Z\", \"created_time\": \"2019-12-18T18:44:31.550863Z\", \"end_time\": \"2019-12-18T18:45:44.506845Z\", \"duration\": \"0:01:12\", \"run_number\": 89005, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-12-18T18:44:31.550863Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2019-12-18 18:44:30Z] Submitting 1 runs, first five are: 279ef49c:fe368603-209a-4056-acf1-4854a1a1e901\\n[2019-12-18 18:45:53Z] Execution of experiment failed, update experiment status and cancel running nodes.\\n\", \"graph\": {\"datasource_nodes\": {}, \"module_nodes\": {\"279ef49c\": {\"node_id\": \"279ef49c\", \"name\": \"logs\", \"status\": \"Failed\", \"_is_reused\": false, \"run_id\": \"fe368603-209a-4056-acf1-4854a1a1e901\"}}, \"edges\": [], \"child_runs\": [{\"run_id\": \"fe368603-209a-4056-acf1-4854a1a1e901\", \"name\": \"logs\", \"status\": \"Failed\", \"start_time\": \"2019-12-18T18:44:50.384413Z\", \"created_time\": \"2019-12-18T18:44:31.550863Z\", \"end_time\": \"2019-12-18T18:45:44.506845Z\", \"duration\": \"0:01:12\", \"run_number\": 89005, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-12-18T18:44:31.550863Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.74\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline2 = Pipeline(workspace=ws, steps=[Python_Script_Step])\n",
    "run2 = experiment.submit(pipeline2)\n",
    "RunDetails(run2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also incorporated logging to collect information about our training pipeline. Each row represents a trained model's record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at our run details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>automl-ojforecasting</td><td>7640a6a8-c383-4627-8a60-ea9a040f8f9a</td><td>azureml.PipelineRun</td><td>Completed</td><td><a href=\"https://ml.azure.com/experiments/automl-ojforecasting/runs/7640a6a8-c383-4627-8a60-ea9a040f8f9a?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: automl-ojforecasting,\n",
       "Id: 7640a6a8-c383-4627-8a60-ea9a040f8f9a,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then download the pipeline output to a local path. In this case, the output is 1 master log file that is aggregated from all the returns of the ParallelRunStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Datastore</th><th>Path on Datastore</th><th>Produced By PipelineRun</th><th>Produced By StepRun</th></tr><tr><td>AllARIMAModels</td><td>workspaceblobstore</td><td>azureml/29e1c57d-6106-4ec8-9dab-b2f8a7d6fbd3/AllARIMAModels</td><td><a href=\"https://ml.azure.com/experiments/automl-ojforecasting/runs/7640a6a8-c383-4627-8a60-ea9a040f8f9a?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\" target=\"_blank\" rel=\"noopener\">7640a6a8-c383-4627-8a60-ea9a040f8f9a</a></td><td><a href=\"https://ml.azure.com/experiments/automl-ojforecasting/runs/29e1c57d-6106-4ec8-9dab-b2f8a7d6fbd3?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\" target=\"_blank\" rel=\"noopener\">29e1c57d-6106-4ec8-9dab-b2f8a7d6fbd3</a></td></tr></table>"
      ],
      "text/plain": [
       "$AZUREML_DATAREFERENCE_AllARIMAModels"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_run = next(run.get_children())\n",
    "prediction_output = prediction_run.get_output_data(\"AllARIMAModels\")\n",
    "prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_output.download(local_path=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/azureml/29e1c57d-6106-4ec8-9dab-b2f8a7d6fbd3/AllARIMAModels/parallel_run_step.txt\n",
      "logs/azureml/4f2a151b-2d69-42a9-8cee-19bdc3fefa98/AllARIMAModels/parallel_run_step.txt\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(\"logs\"):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "            print (result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file='logs/azureml/29e1c57d-6106-4ec8-9dab-b2f8a7d6fbd3/AllARIMAModels/parallel_run_step.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the log file and insert column names. For demonstration, we use 3 models example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Brand</th>\n",
       "      <th>ModelType</th>\n",
       "      <th>FileName</th>\n",
       "      <th>ModelName</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Index</th>\n",
       "      <th>BatchSize</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Store1029</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1029_dominicks</td>\n",
       "      <td>arima_Store1029_dominicks</td>\n",
       "      <td>2019-12-18 22:12:28.624249</td>\n",
       "      <td>2019-12-18 22:12:30.532097</td>\n",
       "      <td>0:00:01.907848</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Store1029</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1029_minute.maid</td>\n",
       "      <td>arima_Store1029_minute.maid</td>\n",
       "      <td>2019-12-18 22:12:30.851295</td>\n",
       "      <td>2019-12-18 22:12:33.198235</td>\n",
       "      <td>0:00:02.346940</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Store1029</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1029_tropicana</td>\n",
       "      <td>arima_Store1029_tropicana</td>\n",
       "      <td>2019-12-18 22:12:33.484467</td>\n",
       "      <td>2019-12-18 22:12:35.667806</td>\n",
       "      <td>0:00:02.183339</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Store1030</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1030_dominicks</td>\n",
       "      <td>arima_Store1030_dominicks</td>\n",
       "      <td>2019-12-18 22:12:35.902769</td>\n",
       "      <td>2019-12-18 22:12:39.266344</td>\n",
       "      <td>0:00:03.363575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Store1030</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1030_minute.maid</td>\n",
       "      <td>arima_Store1030_minute.maid</td>\n",
       "      <td>2019-12-18 22:12:39.455598</td>\n",
       "      <td>2019-12-18 22:12:42.963545</td>\n",
       "      <td>0:00:03.507947</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Store1030</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1030_tropicana</td>\n",
       "      <td>arima_Store1030_tropicana</td>\n",
       "      <td>2019-12-18 22:12:43.145487</td>\n",
       "      <td>2019-12-18 22:12:45.166965</td>\n",
       "      <td>0:00:02.021478</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Store1031</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1031_dominicks</td>\n",
       "      <td>arima_Store1031_dominicks</td>\n",
       "      <td>2019-12-18 22:12:45.373209</td>\n",
       "      <td>2019-12-18 22:12:56.924137</td>\n",
       "      <td>0:00:11.550928</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Store1031</td>\n",
       "      <td>minute.maid</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1031_minute.maid</td>\n",
       "      <td>arima_Store1031_minute.maid</td>\n",
       "      <td>2019-12-18 22:12:57.107645</td>\n",
       "      <td>2019-12-18 22:12:59.119411</td>\n",
       "      <td>0:00:02.011766</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Store1031</td>\n",
       "      <td>tropicana</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1031_tropicana</td>\n",
       "      <td>arima_Store1031_tropicana</td>\n",
       "      <td>2019-12-18 22:12:59.274413</td>\n",
       "      <td>2019-12-18 22:13:00.858137</td>\n",
       "      <td>0:00:01.583724</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Store1032</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Store1032_dominicks</td>\n",
       "      <td>arima_Store1032_dominicks</td>\n",
       "      <td>2019-12-18 22:13:01.002873</td>\n",
       "      <td>2019-12-18 22:13:03.424049</td>\n",
       "      <td>0:00:02.421176</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Running</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Store         Brand ModelType                FileName  \\\n",
       "0  Store1029     dominicks     ARIMA     Store1029_dominicks   \n",
       "1  Store1029   minute.maid     ARIMA   Store1029_minute.maid   \n",
       "2  Store1029     tropicana     ARIMA     Store1029_tropicana   \n",
       "3  Store1030     dominicks     ARIMA     Store1030_dominicks   \n",
       "4  Store1030   minute.maid     ARIMA   Store1030_minute.maid   \n",
       "5  Store1030     tropicana     ARIMA     Store1030_tropicana   \n",
       "6  Store1031     dominicks     ARIMA     Store1031_dominicks   \n",
       "7  Store1031   minute.maid     ARIMA   Store1031_minute.maid   \n",
       "8  Store1031     tropicana     ARIMA     Store1031_tropicana   \n",
       "9  Store1032     dominicks     ARIMA     Store1032_dominicks   \n",
       "\n",
       "                      ModelName                    StartTime  \\\n",
       "0     arima_Store1029_dominicks   2019-12-18 22:12:28.624249   \n",
       "1   arima_Store1029_minute.maid   2019-12-18 22:12:30.851295   \n",
       "2     arima_Store1029_tropicana   2019-12-18 22:12:33.484467   \n",
       "3     arima_Store1030_dominicks   2019-12-18 22:12:35.902769   \n",
       "4   arima_Store1030_minute.maid   2019-12-18 22:12:39.455598   \n",
       "5     arima_Store1030_tropicana   2019-12-18 22:12:43.145487   \n",
       "6     arima_Store1031_dominicks   2019-12-18 22:12:45.373209   \n",
       "7   arima_Store1031_minute.maid   2019-12-18 22:12:57.107645   \n",
       "8     arima_Store1031_tropicana   2019-12-18 22:12:59.274413   \n",
       "9     arima_Store1032_dominicks   2019-12-18 22:13:01.002873   \n",
       "\n",
       "                       EndTime         Duration  Index  BatchSize    Status  \n",
       "0   2019-12-18 22:12:30.532097   0:00:01.907848      0          1   Running  \n",
       "1   2019-12-18 22:12:33.198235   0:00:02.346940      0          1   Running  \n",
       "2   2019-12-18 22:12:35.667806   0:00:02.183339      0          1   Running  \n",
       "3   2019-12-18 22:12:39.266344   0:00:03.363575      0          1   Running  \n",
       "4   2019-12-18 22:12:42.963545   0:00:03.507947      0          1   Running  \n",
       "5   2019-12-18 22:12:45.166965   0:00:02.021478      0          1   Running  \n",
       "6   2019-12-18 22:12:56.924137   0:00:11.550928      0          1   Running  \n",
       "7   2019-12-18 22:12:59.119411   0:00:02.011766      0          1   Running  \n",
       "8   2019-12-18 22:13:00.858137   0:00:01.583724      0          1   Running  \n",
       "9   2019-12-18 22:13:03.424049   0:00:02.421176      0          1   Running  "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(result_file, converters={0: lambda x: x.strip(\"[\"),10: lambda x: x.strip(\"]\")}, delimiter=\",\", header=None)\n",
    "df.columns=['Store','Brand','ModelType','FileName','ModelName','StartTime','EndTime','Duration','Index','BatchSize','Status']\n",
    "df['Store'] = df['Store'].apply(str).str.replace(\"'\", '')\n",
    "df['Brand'] = df['Brand'].apply(str).str.replace(\"'\", '')\n",
    "df['ModelType'] = df['ModelType'].apply(str).str.replace(\"'\", '')\n",
    "df['FileName'] = df['FileName'].apply(str).str.replace(\"'\", '')\n",
    "df['ModelName'] = df['ModelName'].apply(str).str.replace(\"'\", '')\n",
    "df['StartTime'] = df['StartTime'].apply(str).str.replace(\"'\", '')\n",
    "df['EndTime'] = df['EndTime'].apply(str).str.replace(\"'\", '')\n",
    "df['Duration'] = df['Duration'].apply(str).str.replace(\"'\", '')\n",
    "df['Status'] = df['Status'].apply(str).str.replace(\"'\", '')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the log file to a dedicated path in the blob for PBI monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore.upload('logs/azureml/319e0d12-9aad-41c4-9a22-0e85ad11e331/ARIMAmodels/', target_path='traininglogs', overwrite=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add more relevant metrics to logs.\n",
    "2. Save log files to the dedicated path in the training script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

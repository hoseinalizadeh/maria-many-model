{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "We do this using a 'pipeline first mentality' i.e. we want to have a production pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.74\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up workspace, datastore, experiment and compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob manymodelssav16457539585 azureml-blobstore-77752be6-01b4-4a3e-9d42-03c9c0d6248f\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "# auth = InteractiveLoginAuthentication(force=True, tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
    "\n",
    "# set up workspace\n",
    "# ws = Workspace.from_config()\n",
    "ws.get_details()\n",
    "\n",
    "# choose a compute target\n",
    "compute = AmlCompute(ws, \"train-many-model\")\n",
    "\n",
    "# choose a datastore\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "# choose a experiment\n",
    "experiment = Experiment(ws, 'automl-ojforecasting')\n",
    "print(dstore.name, dstore.datastore_type, dstore.account_name, dstore.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up run configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the run config for experiment to run targeting different compute targets in Azure Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['sklearn','pmdarima'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the registered dataset from Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used 12,222 datasets and ParallelRunStep to build 12,222 time-series ARIMA models to predict the quantity of each store brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to register all the datasets in the Workspace first. We uploaded our data to a blob container hence set 'Datastore' as workspaceblobstore 'Relative path' as the correspondig directory in the blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiledst = Dataset.get_by_name(ws, name='Allfiledatasets') \n",
    "allfiledstinput = allfiledst.as_named_input('trainallmodels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment defines a collection of resources that we will need to run our Azure pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ParallelRunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "workercount=2\n",
    "nodecount=5\n",
    "timeout=3000\n",
    "\n",
    "output_dir = PipelineData(name=\"ARIMAmodels\", \n",
    "                          datastore=dstore, \n",
    "                          output_path_on_compute=\"ARIMAmodels/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname='store'\n",
    "\n",
    "tags1={}\n",
    "tags1['dataset']=datasetname\n",
    "tags1['nodes']=nodecount\n",
    "tags1['workers-per-node']=workercount\n",
    "tags1['timeout']=timeout\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='./scripts',\n",
    "    entry_script='train.py',\n",
    "    mini_batch_size=\"5\",\n",
    "    run_invocation_timeout=timeout,\n",
    "    error_threshold=10,\n",
    "    output_action=\"summary_only\",\n",
    "    environment=batch_env,\n",
    "    process_count_per_node=workercount,\n",
    "    compute_target=compute,\n",
    "    node_count=nodecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ParallelRunStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added 3 arguments that users can customize based on the prediction goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-training\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[allfiledstinput],\n",
    "    output=output_dir,\n",
    "    models=[],\n",
    "    arguments=['--target_column','Quantity', '--n_test_periods',6, '--timestamp_column','WeekStarting'],\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step many-models-training [28811bee][794039ef-279e-47b6-9fe6-6c5d0b0f5e20], (This step will run and generate new outputs)\n",
      "Using data reference trainallmodels_0 for StepId [e5ddafbd][2f363fdf-cc44-4e00-b172-099c1b4048f3], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun c8a4559a-aac6-4985-ae63-f9e18e29a9ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Converting non-string tag to string: (nodes: 5)\n",
      "WARNING - Converting non-string tag to string: (workers-per-node: 2)\n",
      "WARNING - Converting non-string tag to string: (timeout: 3000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/automl-ojforecasting/runs/c8a4559a-aac6-4985-ae63-f9e18e29a9ca?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d596734c091f42ee923ac7f58d678e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/automl-ojforecasting/runs/c8a4559a-aac6-4985-ae63-f9e18e29a9ca?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\", \"run_id\": \"c8a4559a-aac6-4985-ae63-f9e18e29a9ca\", \"run_properties\": {\"run_id\": \"c8a4559a-aac6-4985-ae63-f9e18e29a9ca\", \"created_utc\": \"2019-12-05T22:52:53.373744Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": null, \"runType\": \"HTTP\", \"azureml.parameters\": \"{\\\"aml_process_count_per_node\\\":\\\"2\\\",\\\"aml_node_count\\\":\\\"5\\\"}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\", \"dataset\": \"store\", \"nodes\": \"5\", \"workers-per-node\": \"2\", \"timeout\": \"3000\"}, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://manymodelssav16457539585.blob.core.windows.net/azureml/ExperimentRun/dcid.c8a4559a-aac6-4985-ae63-f9e18e29a9ca/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=EMRzQ4YeCAtegCx6LXCjXv1IxZHjdeS5Y3biL8qlYws%3D&st=2019-12-05T22%3A48%3A11Z&se=2019-12-06T06%3A58%3A11Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://manymodelssav16457539585.blob.core.windows.net/azureml/ExperimentRun/dcid.c8a4559a-aac6-4985-ae63-f9e18e29a9ca/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=s2Kft9H%2BcWogL%2FSv7LKJlBWQSi%2BMNOgQ1EC1U5kFwNc%3D&st=2019-12-05T22%3A48%3A11Z&se=2019-12-06T06%3A58%3A11Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://manymodelssav16457539585.blob.core.windows.net/azureml/ExperimentRun/dcid.c8a4559a-aac6-4985-ae63-f9e18e29a9ca/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=ZZ%2BALFeHFVhxPYsT4FZ%2BlJyL4M0PqPnLTLM2P1tiUeE%3D&st=2019-12-05T22%3A48%3A11Z&se=2019-12-06T06%3A58%3A11Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:04:41\"}, \"child_runs\": [{\"run_id\": \"e728447b-4532-4802-b9bc-5b637c942124\", \"name\": \"many-models-training\", \"status\": \"Running\", \"start_time\": \"\", \"created_time\": \"2019-12-05T22:53:21.694616Z\", \"end_time\": \"\", \"duration\": \"0:04:15\", \"run_number\": 3979, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-12-05T22:53:21.694616Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2019-12-05 22:53:20Z] Submitting run id e728447b-4532-4802-b9bc-5b637c942124 in experiment automl-ojforecasting\\n\", \"graph\": {\"datasource_nodes\": {\"e5ddafbd\": {\"node_id\": \"e5ddafbd\", \"name\": \"trainallmodels_0\"}}, \"module_nodes\": {\"28811bee\": {\"node_id\": \"28811bee\", \"name\": \"many-models-training\", \"status\": \"Running\", \"_is_reused\": false, \"run_id\": \"e728447b-4532-4802-b9bc-5b637c942124\"}}, \"edges\": [{\"source_node_id\": \"e5ddafbd\", \"source_node_name\": \"trainallmodels_0\", \"source_name\": \"data\", \"target_name\": \"trainallmodels_0\", \"dst_node_id\": \"28811bee\", \"dst_node_name\": \"many-models-training\"}], \"child_runs\": [{\"run_id\": \"e728447b-4532-4802-b9bc-5b637c942124\", \"name\": \"many-models-training\", \"status\": \"Running\", \"start_time\": \"\", \"created_time\": \"2019-12-05T22:53:21.694616Z\", \"end_time\": \"\", \"duration\": \"0:04:15\", \"run_number\": 3979, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2019-12-05T22:53:21.694616Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.74\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "\n",
    "run = experiment.submit(pipeline,tags=tags1)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: c8a4559a-aac6-4985-ae63-f9e18e29a9ca\n",
      "Link to Portal: https://ml.azure.com/experiments/automl-ojforecasting/runs/c8a4559a-aac6-4985-ae63-f9e18e29a9ca?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: e728447b-4532-4802-b9bc-5b637c942124\n",
      "Link to Portal: https://ml.azure.com/experiments/automl-ojforecasting/runs/e728447b-4532-4802-b9bc-5b637c942124?wsid=/subscriptions/bbd86e7d-3602-4e6d-baa4-40ae2ad9303c/resourcegroups/ManyModelsSA/workspaces/ManyModelsSAv1\n",
      "StepRun( many-models-training ) Status: NotStarted\n",
      "StepRun( many-models-training ) Status: Queued\n",
      "StepRun( many-models-training ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_6dbcb125e056bc2309a432f4037b753f10fa4069e73437bbf52b7c4fb49822f1_d.txt\n",
      "========================================================================================================================\n",
      "2019-12-05T23:00:14Z Starting output-watcher...\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_b55ccba3e015ef6ca93b6a296bf3a2b2\n",
      "a1298f4ce990: Pulling fs layer\n",
      "04a3282d9c4b: Pulling fs layer\n",
      "9b0d3db6dc03: Pulling fs layer\n",
      "8269c605f3f1: Pulling fs layer\n",
      "6504d449e70c: Pulling fs layer\n",
      "4e38f320d0d4: Pulling fs layer\n",
      "b0a763e8ee03: Pulling fs layer\n",
      "11917a028ca4: Pulling fs layer\n",
      "a6c378d11cbf: Pulling fs layer\n",
      "6cc007ad9140: Pulling fs layer\n",
      "6c1698a608f3: Pulling fs layer\n",
      "6366198a1ae1: Pulling fs layer\n",
      "f6336f04428a: Pulling fs layer\n",
      "6df35e9edcda: Pulling fs layer\n",
      "35fb444532d7: Pulling fs layer\n",
      "8269c605f3f1: Waiting\n",
      "6504d449e70c: Waiting\n",
      "4e38f320d0d4: Waiting\n",
      "de22ffed9b0e: Pulling fs layer\n",
      "77e1315ad91b: Pulling fs layer\n",
      "b0a763e8ee03: Waiting\n",
      "11917a028ca4: Waiting\n",
      "a6c378d11cbf: Waiting\n",
      "6c1698a608f3: Waiting\n",
      "6cc007ad9140: Waiting\n",
      "6366198a1ae1: Waiting\n",
      "f6336f04428a: Waiting\n",
      "6df35e9edcda: Waiting\n",
      "35fb444532d7: Waiting\n",
      "77e1315ad91b: Waiting\n",
      "de22ffed9b0e: Waiting\n",
      "9b0d3db6dc03: Verifying Checksum\n",
      "9b0d3db6dc03: Download complete\n",
      "04a3282d9c4b: Verifying Checksum\n",
      "04a3282d9c4b: Download complete\n",
      "8269c605f3f1: Verifying Checksum\n",
      "8269c605f3f1: Download complete\n",
      "a1298f4ce990: Verifying Checksum\n",
      "a1298f4ce990: Download complete\n",
      "6504d449e70c: Verifying Checksum\n",
      "6504d449e70c: Download complete\n",
      "4e38f320d0d4: Verifying Checksum\n",
      "4e38f320d0d4: Download complete\n",
      "b0a763e8ee03: Verifying Checksum\n",
      "b0a763e8ee03: Download complete\n",
      "6cc007ad9140: Verifying Checksum\n",
      "6cc007ad9140: Download complete\n",
      "6c1698a608f3: Verifying Checksum\n",
      "6c1698a608f3: Download complete\n",
      "6366198a1ae1: Verifying Checksum\n",
      "6366198a1ae1: Download complete\n",
      "11917a028ca4: Download complete\n",
      "f6336f04428a: Verifying Checksum\n",
      "f6336f04428a: Download complete\n",
      "6df35e9edcda: Verifying Checksum\n",
      "6df35e9edcda: Download complete\n",
      "35fb444532d7: Verifying Checksum\n",
      "35fb444532d7: Download complete\n",
      "77e1315ad91b: Download complete\n",
      "a6c378d11cbf: Verifying Checksum\n",
      "a6c378d11cbf: Download complete\n",
      "a1298f4ce990: Pull complete\n",
      "04a3282d9c4b: Pull complete\n",
      "9b0d3db6dc03: Pull complete\n",
      "8269c605f3f1: Pull complete\n",
      "de22ffed9b0e: Download complete\n",
      "6504d449e70c: Pull complete\n",
      "4e38f320d0d4: Pull complete\n",
      "b0a763e8ee03: Pull complete\n",
      "11917a028ca4: Pull complete\n",
      "a6c378d11cbf: Pull complete\n",
      "6cc007ad9140: Pull complete\n",
      "6c1698a608f3: Pull complete\n",
      "6366198a1ae1: Pull complete\n",
      "f6336f04428a: Pull complete\n",
      "6df35e9edcda: Pull complete\n",
      "35fb444532d7: Pull complete\n",
      "de22ffed9b0e: Pull complete\n",
      "77e1315ad91b: Pull complete\n",
      "Digest: sha256:654641f9635f9af41169636a6cf066cccf96c70fcdc1591c9be169efdc3ba2f3\n",
      "Status: Downloaded newer image for manymodelssa8faf85d6.azurecr.io/azureml/azureml_b55ccba3e015ef6ca93b6a296bf3a2b2:latest\n",
      "3422595aec4a4f5d4141f43ea0820739012b78f0edf67c8cb393ab6cd23b2000\n",
      "2019/12/05 23:01:05 Version: 3.0.01032.0003 Branch: master Commit: 2bbedd1a\n",
      "2019/12/05 23:01:05 sshd runtime has already been installed in the container\n",
      "ssh-keygen: /azureml-envs/azureml_c46313de5fc6278ee028076ebb69e934/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "ssh-keygen: /azureml-envs/azureml_c46313de5fc6278ee028076ebb69e934/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_6dbcb125e056bc2309a432f4037b753f10fa4069e73437bbf52b7c4fb49822f1_d.txt\n",
      "===============================================================================================================\n",
      "bash: /azureml-envs/azureml_c46313de5fc6278ee028076ebb69e934/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Not a master node. Exiting.\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "bash: /azureml-envs/azureml_c46313de5fc6278ee028076ebb69e934/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "bash: /azureml-envs/azureml_c46313de5fc6278ee028076ebb69e934/lib/libtinfo.so.5: no version information available (required by bash)\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 157\n",
      "Entering Run History Context Manager.\n"
     ]
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/train.py\n",
    "\n",
    "from azureml.core.run import Run\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from entry_script_helper import EntryScriptHelper\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "thisrun = Run.get_context()\n",
    "\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "print(\"Split the data into train and test\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--target_column\", type=str, help=\"input target column\")\n",
    "parser.add_argument(\"--n_test_periods\", type=int, help=\"input number of test periods\")\n",
    "parser.add_argument(\"--timestamp_column\", type=str, help=\"input timestamp column\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(\"Argument 1(n_test_periods): %s\" % args.n_test_periods)\n",
    "print(\"Argument 2(target_column): %s\" % args.target_column)\n",
    "print(\"Argument 3(timestamp_column): %s\" % args.timestamp_column)\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")\n",
    "    return\n",
    "\n",
    "def run(input_data):\n",
    "    # 0. Set up logging\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    resultList = []\n",
    "    logger.info('processing all files')\n",
    "\n",
    "    # 1. Read in the data file\n",
    "    for idx, csv_file_path in enumerate(input_data):\n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]\n",
    "        with thisrun.child_run(name=mname) as childrun:\n",
    "            for w in range(0,5):\n",
    "                thisrun.log(mname,str(w))\n",
    "            date1=datetime.datetime.now()\n",
    "            logger.info('starting ('+csv_file_path+') ' + str(date1))\n",
    "            childrun.log(mname,'starttime-'+str(date1))\n",
    "\n",
    "            data = pd.read_csv(csv_file_path,header=0)\n",
    "            logger.info(data.head())\n",
    "\n",
    "            # 2. Split the data into train and test sets based on dates\n",
    "            data = data.set_index(args.timestamp_column)\n",
    "            max_date = datetime.datetime.strptime(data.index.max(),'%Y-%m-%d')\n",
    "            split_date = max_date - timedelta(days=7*args.n_test_periods)\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            train = data[data.index <= split_date]\n",
    "            test = data[data.index > split_date]\n",
    "\n",
    "            # 3.Train the model\n",
    "            model = pm.auto_arima(train[args.target_column],\n",
    "                      start_p=0,\n",
    "                      start_q=0,\n",
    "                      test='adf', #default stationarity test is kpps\n",
    "                      max_p =3,\n",
    "                      max_d = 2,\n",
    "                      max_q=3,\n",
    "                      m=3, #number of observations per seasonal cycle\n",
    "                      #d=None,\n",
    "                      seasonal=True,\n",
    "                      #trend = None, # adjust this if the series have trend\n",
    "                      #start_P=0,\n",
    "                      #D=0,\n",
    "                      information_criterion = 'aic',\n",
    "                      trace=True, #prints status on the fits\n",
    "                      #error_action='ignore',\n",
    "                      stepwise = False, # this increments instead of doing a grid search\n",
    "                      suppress_warnings = True,\n",
    "                      out_of_sample_size = 16\n",
    "                     )\n",
    "            model = model.fit(train[args.target_column])\n",
    "            logger.info('done training')\n",
    "\n",
    "            # 4. Save the model\n",
    "            logger.info(model)\n",
    "            logger.info(mname)\n",
    "            with open(mname, 'wb') as file:\n",
    "                joblib.dump(value=model, filename=os.path.join('./outputs/', mname))\n",
    "\n",
    "            # 5. Register the model to the workspace\n",
    "            ws1 = childrun.experiment.workspace\n",
    "            try:\n",
    "                childrun.upload_file(mname, os.path.join('./outputs/', mname))\n",
    "            except:\n",
    "                logger.info('dont need to upload')\n",
    "            logger.info('register model, skip the outputs prefix')\n",
    "            Model.register(workspace=ws1, model_path=os.path.join('./outputs/', mname), model_name='arima_'+str(input_data).split('/')[-1][:-6], model_framework='pmdarima')\n",
    "            date2=datetime.datetime.now()\n",
    "            logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "            #6. Log some metrics\n",
    "            childrun.log(mname,'endtime-'+str(date2))\n",
    "            childrun.log(mname,'auc-1')\n",
    "        resultList.append(True)\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There're multiple arima_Store5_tropicana, arima_Store2_dominicks, arima_Store8_minute.maid models are built, but there is only 1 dataset for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Conduct performance testing/analysis - determine optimal values for parameters like node_count, process_count_per_node, workercount\n",
    "2. Understand and incorporate tags, tags may contain information about model name etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

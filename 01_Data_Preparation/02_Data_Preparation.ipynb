{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation\n",
    "\n",
    "We will be leveraging Azure Open Datasets to pull in the Orange Juice Sales Data. We are going to forecast weekly quantity of orange juice sold for each Brand at each Store. \n",
    "\n",
    "The data used in this example is from the University of Chicago's Dominick's Finer Foods dataset to forecast orange juice sales. Dominick's was a grocery chain in the Chicago metropolitan area. \n",
    "\n",
    "This Notebook will walk through the following steps: \n",
    "- Pulling down the data locally. \n",
    "- Grouping the data by Store and Brand. \n",
    "- Uploading the data to Azure Blob Storage.\n",
    "- Registering a File Dataset to the Workspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites \n",
    "This example runs on an Azure Machine Learning Notebook VM. If you have already run the Environment Setup notebook or you have an AML Workspace and Datastore set up you are all set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the workspace and datastore\n",
    "From the Environment Setup notebook, we need to call the Workspace and Datastore we set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (in progress) Leverage Open Datasets to load the Orange Juice Sales data\n",
    "We will use Open Datasets to pull in the subset of data we want to work with. You can set the sample size to determine the number of series you would like to use to build models. (add in what the max is) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold for pulling data from open datasets \n",
    "\n",
    "#from azureml.opendatasets import OjSales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporarily reading data from local directory\n",
    "Currently, we are reading the data from our local directory and selecting the subset that we would like to build models off of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = 'WeekStarting'\n",
    "data = pd.read_csv(\"generated_oj_sales.csv\", parse_dates=[time_column_name])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample size can be adjusted to run different numbers of models. We are subsetting our data by the store numbers. The maximum number of stores is 3,991. If you would like to run models for all 11,973 series you may skip the subsetting step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample_size = 10\n",
    "store_list = data['Store'].unique().tolist()\n",
    "\n",
    "# Pull a random subset of stores\n",
    "store_sample = random.sample(store_list, sample_size)\n",
    "print(store_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the table to the subset \n",
    "oj_sales_raw = data[data['Store'].isin(store_sample)]\n",
    "print(len(oj_sales_raw['Store'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into groups \n",
    "We want to group our data by Store and Brand. This is the level we would like to forecast quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by store and brand \n",
    "store_brand_groups = [x for _, x in oj_sales_raw.groupby(['Store', 'Brand'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data locally \n",
    "In order to upload the data to Blob, we need to save it locally. We will create a directory to write the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data Directory in local path\n",
    "data_dir = \"data\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "# Create a folder for the oj_sales data \n",
    "oj_sales_path = data_dir + \"/oj_sales_data\"\n",
    "\n",
    "if not os.path.exists(oj_dir):\n",
    "    os.mkdir(oj_dir)\n",
    "\n",
    "\n",
    "# save each store/brand to csv to upload \n",
    "for grp in store_brand_groups: \n",
    "    file_name = '/store' + str(grp['Store'].unique()).lstrip(\"['\").rstrip(\"']\") + '_' + \n",
    "        str(grp['Brand'].unique()).lstrip(\"['\").rstrip(\"']\") + '.csv'\n",
    "        \n",
    "    grp.to_csv(path_or_buf = oj_sales_path + file_name, index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the individual datasets to Blob Storage\n",
    "We upload the data to Blob and will create the FileDataset from this folder of csv files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = 'oj_sales_data' + str(sample_size*3)\n",
    "\n",
    "datastore.upload(src_dir = oj_sales_path,\n",
    "                target_path = target_path,\n",
    "                overwrite = True, \n",
    "                show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the file dataset \n",
    "We need to define the path of the data to create the [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "ds_name = 'oj_data_'+ str(sample_size*3)\n",
    "path_on_datastore = datastore.path(target_path + '/')\n",
    "\n",
    "input_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the file dataset to the workspace \n",
    "We want to register the dataset to our workspace so we can call it as an input into our Pipeline for forecasting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_ds = input_ds.register(ws, ds_name, create_new_version=True)\n",
    "named_ds = registered_ds.as_named_input(ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Resigstered dataset\n",
    "After reigstering the data, we can call it into our Notebook. We will use this in the Training and Scoring Notebooks. You can also download the data from the FileDataset back to a local directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_ds = Dataset.get_by_name(ws, name = ds_name)\n",
    "oj_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data locally \n",
    "oj_ds.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

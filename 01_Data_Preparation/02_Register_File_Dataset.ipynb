{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation\n",
    "\n",
    "We will be leveraging Azure Open Datasets to pull in the Orange Juice Sales Data. \n",
    "\n",
    "This Notebook will walk through the following steps: \n",
    "- Pulling down the data locally. \n",
    "- Grouping the data by Store and Brand. \n",
    "- Uploading the data to Azure Blob Storage.\n",
    "- Registering a File Dataset to the Workspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the workspace and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage Open Datasets to load the Orange Juice Sales data\n",
    "We will use Open Datasets to pull in the subset of data we want to work with. You can set the sample size to determine the number of series you would like to use to build models. (add in what the max is) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold for pulling data from open datasets \n",
    "\n",
    "#from azureml.opendatasets import OjSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add filtering to what part of the oj data you want to bring in \n",
    "sample_size = 10\n",
    "\n",
    "# loop to pull in correct # series \n",
    "\n",
    "oj_sales_raw = # add in code here from subset pull \n",
    "\n",
    "## NY Taxi Open Source Data for reference \n",
    "#for sample_month in range(number_of_months):\n",
    "#    temp_df_green = NycTlcGreen(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "#        .to_pandas_dataframe()\n",
    "#    green_df_raw = green_df_raw.append(temp_df_green.sample(sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by store and brand \n",
    "store_brand_groups = [x for _, x in oj_sales_raw.groupby(['Store', 'Brand'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data Directory in local path\n",
    "data_dir = \"data\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "# Create a folder for the oj_sales data \n",
    "oj_sales_path = data_dir + \"/oj_sales_data\"\n",
    "\n",
    "if not os.path.exists(oj_dir):\n",
    "    os.mkdir(oj_dir)\n",
    "\n",
    "\n",
    "# save each store/brand to csv to upload \n",
    "for grp in store_brand_groups: \n",
    "    file_name = '/store' + str(grp['Store'].unique()).lstrip(\"['\").rstrip(\"']\") + '_' + \n",
    "        str(grp['Brand'].unique()).lstrip(\"['\").rstrip(\"']\") + '.csv'\n",
    "        \n",
    "    grp.to_csv(path_or_buf = oj_sales_path + file_name, index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the individual datasets to Blob Storage\n",
    "We will create the FileDataset from this folder of csv files on Blob. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to blob \n",
    "target_path = 'oj_sales_data'\n",
    "\n",
    "datastore.upload(src_dir = oj_sales_path,\n",
    "                target_path = target_path,\n",
    "                overwrite = True, \n",
    "                show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the file dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "ds_name = 'oj_data'\n",
    "path_on_datastore = datastore.path(target_path + '/')\n",
    "\n",
    "input_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the file dataset to the workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_ds = input_ds.register(ws, ds_name, create_new_version=True)\n",
    "named_ds = registered_ds.as_named_input(ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Resigstered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_ds = Dataset.get_by_name(ws, name = ds_name)\n",
    "\n",
    "df = oj_ds.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

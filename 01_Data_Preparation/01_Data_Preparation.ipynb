{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation\n",
    "\n",
    "We will be leveraging Azure Open Datasets to pull in the Orange Juice Sales Data. We are going to forecast weekly quantity of orange juice sold for each Brand at each Store. \n",
    "\n",
    "The data used in this example is from the University of Chicago's Dominick's Finer Foods dataset to forecast orange juice sales. Dominick's was a grocery chain in the Chicago metropolitan area. \n",
    "\n",
    "#### This Notebook will walk through the following steps: \n",
    "- Pulling the data locally \n",
    "- Grouping the data by Store and Brand \n",
    "- Uploading the data to Azure Blob Storage\n",
    "- Registering a File Dataset to the Workspace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites \n",
    "This example runs on an Azure Machine Learning Notebook VM. If you have already run the Environment Setup notebook or you have an AML Workspace and Datastore set up you are all set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the workspace and datastore\n",
    "From the Environment Setup notebook, we need to call the Workspace and Datastore we set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace(subscription_id=\"bbd86e7d-3602-4e6d-baa4-40ae2ad9303c\", resource_group=\"ManyModelsSA\", workspace_name=\"ManyModelsSAv1\")\n",
    "# ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (in progress) Leverage Open Datasets to load the Orange Juice Sales data\n",
    "We will use Open Datasets to pull in the subset of data we want to work with. You can set the sample size to determine the number of series you would like to use to build models. (add in what the max is) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold for pulling data from open datasets \n",
    "\n",
    "#from azureml.opendatasets import OjSales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporarily reading data from local directory\n",
    "Currently, we are reading the data from our local directory and selecting the subset that we would like to build models off of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_column_name = 'WeekStarting'\n",
    "data = pd.read_csv(\"generated_oj_sales.csv\", parse_dates=[time_column_name])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into groups \n",
    "We want to group our data by Store and Brand. This is the level we would like to forecast quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by store and brand \n",
    "store_brand_groups = [x for _, x in oj_sales_raw.groupby(['Store', 'Brand'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data locally \n",
    "In order to upload the data to Blob, we need to save it locally. We will create a directory to write the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data Directory in local path\n",
    "data_dir = \"data\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "# Create a folder for the oj_sales data \n",
    "oj_sales_path = data_dir + \"/oj_sales_data\"\n",
    "\n",
    "if not os.path.exists(oj_dir):\n",
    "    os.mkdir(oj_dir)\n",
    "\n",
    "\n",
    "# save each store/brand to csv to upload \n",
    "for grp in store_brand_groups: \n",
    "    file_name = '/store' + str(grp['Store'].unique()).lstrip(\"['\").rstrip(\"']\") + '_' + \n",
    "        str(grp['Brand'].unique()).lstrip(\"['\").rstrip(\"']\") + '.csv'\n",
    "        \n",
    "    grp.to_csv(path_or_buf = oj_sales_path + file_name, index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the individual datasets to Blob Storage\n",
    "We upload the data to Blob and will create the FileDataset from this folder of csv files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = 'oj_sales_data' + str(sample_size*3)\n",
    "\n",
    "datastore.upload(src_dir = oj_sales_path,\n",
    "                target_path = target_path,\n",
    "                overwrite = True, \n",
    "                show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the file dataset \n",
    "We need to define the path of the data to create the [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "ds_name = 'oj_data'\n",
    "path_on_datastore = datastore.path(target_path + '/')\n",
    "\n",
    "input_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the file dataset to the workspace \n",
    "We want to register the dataset to our workspace so we can call it as an input into our Pipeline for forecasting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_ds = input_ds.register(ws, ds_name, create_new_version=True)\n",
    "named_ds = registered_ds.as_named_input(ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Resigstered dataset\n",
    "After reigstering the data, we can call it into our Notebook. We will use this in the Training and Scoring Notebooks. You can also download the data from the FileDataset back to a local directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_ds = Dataset.get_by_name(ws, name = ds_name)\n",
    "oj_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data locally \n",
    "oj_ds.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset a small number of datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate how to subset 9 datasets - 3 stores for 3 different brands in the following blocks. You can change it to any number you'd like through changing the number in the take() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'AllDataProd/')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"13d13057-951b-45e4-8c6f-ced91644781c\",\n",
       "    \"name\": \"AllDataProd\",\n",
       "    \"version\": 2,\n",
       "    \"workspace\": \"Workspace.create(name='ManyModelsSAv1', subscription_id='bbd86e7d-3602-4e6d-baa4-40ae2ad9303c', resource_group='ManyModelsSA')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "FileDstAllModels = Dataset.get_by_name(ws, name='AllDataProd')\n",
    "FileDstAllModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileDst9Models = FileDstAllModels.take(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to_path() method will print out the subsetted file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/Store1000_dominicks.csv', '/Store1000_minute.maid.csv',\n",
       "       '/Store1000_tropicana.csv', '/Store1001_dominicks.csv',\n",
       "       '/Store1001_minute.maid.csv', '/Store1001_tropicana.csv',\n",
       "       '/Store1002_dominicks.csv', '/Store1002_minute.maid.csv',\n",
       "       '/Store1002_tropicana.csv'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileDst9Models.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a temporary directory to download the 9 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_dominicks.csv',\n",
       "       '/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_minute.maid.csv',\n",
       "       '/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_tropicana.csv',\n",
       "       '/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_dominicks.csv',\n",
       "       '/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_minute.maid.csv',\n",
       "       '/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_tropicana.csv',\n",
       "       '/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_dominicks.csv',\n",
       "       '/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_minute.maid.csv',\n",
       "       '/var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_tropicana.csv'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "data_folder = tempfile.mkdtemp()\n",
    "FileDst9Models.download(data_folder, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the dowonlaoded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Store1000_dominicks.csv',\n",
       " 'Store1002_minute.maid.csv',\n",
       " 'Store1000_tropicana.csv',\n",
       " 'Store1001_minute.maid.csv',\n",
       " 'Store1001_tropicana.csv',\n",
       " 'Store1002_dominicks.csv',\n",
       " 'Store1000_minute.maid.csv',\n",
       " 'Store1001_dominicks.csv',\n",
       " 'Store1002_tropicana.csv']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the downloaded files to the blob container and named the folder 'oj_sales_9_datasets'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 9 files\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_dominicks.csv\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_minute.maid.csv\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_tropicana.csv\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_dominicks.csv\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_minute.maid.csv\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_tropicana.csv\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_dominicks.csv\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_minute.maid.csv\n",
      "Uploading /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_tropicana.csv\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_tropicana.csv, 1 files out of an estimated total of 9\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_dominicks.csv, 2 files out of an estimated total of 9\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_tropicana.csv, 3 files out of an estimated total of 9\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1002_minute.maid.csv, 4 files out of an estimated total of 9\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_minute.maid.csv, 5 files out of an estimated total of 9\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_minute.maid.csv, 6 files out of an estimated total of 9\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_tropicana.csv, 7 files out of an estimated total of 9\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1000_dominicks.csv, 8 files out of an estimated total of 9\n",
      "Uploaded /var/folders/ds/k3qt7v792nz1vf7vxm0vwbz5q68fz8/T/tmp9wywgj79/Store1001_dominicks.csv, 9 files out of an estimated total of 9\n",
      "Uploaded 9 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_cbaac2c516464edf9511efe7e0dd41d6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_path = 'oj_sales_9_datasets'\n",
    "\n",
    "datastore.upload(src_dir = data_folder,\n",
    "                target_path = target_path,\n",
    "                overwrite = True, \n",
    "                show_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then register the file datasets to the Workspace and name it as 'FileDst9Models'.  We can call this registered dataset later in our notebook. This is one-time set up that you don't need to re-run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_paths = [(datastore, target_path)]\n",
    "ojsales9_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "ojsales9_ds = ojsales9_ds.register(workspace=ws, \n",
    "                                       name='FileDst9Models', \n",
    "                                       description='9 files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'oj_sales_9_datasets')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"bd170b79-d43f-4f76-8ad2-0831f5e7c28e\",\n",
       "    \"name\": \"FileDst9Models\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"9 files\",\n",
       "    \"workspace\": \"Workspace.create(name='ManyModelsSAv1', subscription_id='bbd86e7d-3602-4e6d-baa4-40ae2ad9303c', resource_group='ManyModelsSA')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ojsales9_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we delete the temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset datasets without explicitly cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "target_path = 'oj_sales_9_datasets'\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    FileDst9Models.download(temp_dir, overwrite=True)\n",
    "    datastore.upload(src_dir = temp_dir,\n",
    "                target_path = target_path,\n",
    "                overwrite = True, \n",
    "                show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_paths = [(datastore, target_path)]\n",
    "ojsales9_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "ojsales9_ds = ojsales9_ds.register(workspace=ws, \n",
    "                                       name='FileDst9Models', \n",
    "                                       description='9 files')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

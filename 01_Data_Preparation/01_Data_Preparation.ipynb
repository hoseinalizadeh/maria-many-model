{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Data Preparation\n",
    "---\n",
    "\n",
    "This solution accelerator uses simulated orange juice sales data to walk you through the process of training many models on Azure Machine Learning. \n",
    "\n",
    "The time series data used in this example was simulated based on the University of Chicago's Dominick's Finer Foods dataset which featured two years of sales of 3 different orange juice brands for individual stores. The full simulated dataset includes 3,991 stores with 3 orange juice brands each thus allowing 11,973 models to be trained to showcase the power of the many models pattern.\n",
    "\n",
    "  \n",
    "In this notebook, two datasets will be created: one with all 11,973 files and one with only 10 files that can be used to quickly test and debug. For each dataset, you'll walk you through the process of:\n",
    "\n",
    "1. Downloading the data from Azure Open Datasets\n",
    "2. Uploading the data to Datastore\n",
    "3. Registering a File Dataset to the Workspace\n",
    "\n",
    "\n",
    "### Prerequisites \n",
    "At this point, you should have already: \n",
    "1. Created your AML Workspace\n",
    "2. Run 00_Environment_Setup.ipynb to configure the enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Connect to your workspace and datastore\n",
    "In the Environemnet Setup notebook you created a [Workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py). We are going to use that enviroment to register the data. You also set up the Datastore which in this example is a container in Blob storage where we will store the data. The Datastore object contains the connection to the storage location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace Name: ManyModelsAccelerator\n",
      "Azure Region: westus2\n",
      "Subscription Id: f97fb87f-32d7-4d7c-9bc5-ea43b4fea7ac\n",
      "Resource Group: ManyModelsRG\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "print('Workspace Name: ' + ws.name, \n",
    "      'Azure Region: ' + ws.location, \n",
    "      'Subscription Id: ' + ws.subscription_id, \n",
    "      'Resource Group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Download the data from Azure Open Datasets\n",
    "To download the data, import OjSalesSimulated from Azure Open Datasets. Two datasets are being created: one with all 11,973 files and one with 10 files. The smaller dataset is designed to enable you to quickly test and debug the notebooks. This can be customized based on your preferences.\n",
    "\n",
    "To use your own data, create a local folder with each time series as a seperate file. Then use that folder and directory in section 3.0 to upload your data to the Datastore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azureml-opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import OjSalesSimulated\n",
    "\n",
    "# Pull all of the data\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\n",
    "\n",
    "# Pull the first 10 files\n",
    "oj_sales_files_small = OjSalesSimulated.get_file_dataset().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are recieving an error importing OjSalesSimulated, run the following command and then *restart the kernal*:\n",
    "```\n",
    "!pip install azureml-opendatasets==0.1.0.8487238 --extra-index-url https://azuremlsdktestpypi.azureedge.net/CLI-SDK-Runners-Validation/8487238/\n",
    "```\n",
    "\n",
    "Next, create the folders that the data will be downloaded to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#oj_sales_path = \"oj_sales_data\"\n",
    "#if not os.path.exists(oj_sales_path):\n",
    "#    os.mkdir(oj_sales_path)\n",
    "    \n",
    "oj_sales_path_small = \"oj_sales_data_small\"\n",
    "if not os.path.exists(oj_sales_path_small):\n",
    "    os.mkdir(oj_sales_path_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, download the files to the folder you created. This may take a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1000_dominicks.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1000_minute.maid.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1000_tropicana.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1001_dominicks.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1001_minute.maid.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1001_tropicana.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1002_dominicks.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1002_minute.maid.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1002_tropicana.csv',\n",
       " 'E:\\\\solution-accelerator-many-models\\\\01_Data_Preparation\\\\oj_sales_data_small\\\\Store1003_dominicks.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oj_sales_files.download(oj_sales_path, overwrite=True)\n",
    "oj_sales_files_small.download(oj_sales_path_small, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Upload the files to your datastore\n",
    "To create the [FileDataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) needed for the ParallelRunStep, you first need to upload the csv files to your blob datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_0bddada51b6d48d9b96ede3ab942dbca"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "target_path = 'oj_sales_data' \n",
    "datastore.upload(src_dir = oj_sales_path,\n",
    "                target_path = target_path,\n",
    "                overwrite = True, \n",
    "                show_progress = False)\n",
    "'''\n",
    "target_path_small = 'oj_sales_data_small'\n",
    "datastore.upload(src_dir = oj_sales_path_small,\n",
    "                target_path = target_path_small,\n",
    "                overwrite = True, \n",
    "                show_progress = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Create the FileDatasets \n",
    "\n",
    "Now that the files exist in the datastore, [FileDatasets](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.file_dataset.filedataset?view=azure-ml-py) can be created. Datasets in Azure Machine Learning are references to specific data in a datastore.  We are using FileDatasets since we are storing each series as a seperate file. This will help to seperate the data needed for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "'''\n",
    "ds_name = 'oj_data'\n",
    "path_on_datastore = datastore.path(target_path + '/')\n",
    "input_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)\n",
    "'''\n",
    "\n",
    "ds_name_small = 'oj_data_small'\n",
    "path_on_datastore_small = datastore.path(target_path_small + '/')\n",
    "input_ds_small = Dataset.File.from_files(path=path_on_datastore_small, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Register the FileDataSets to the workspace \n",
    "Finally, register the dataset to your Workspace so it can be called as an input into the training pipeline in the next notebook. This same dataset will also be used as part of the scoring and forecasting pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#registered_ds = input_ds.register(ws, ds_name, create_new_version=True)\n",
    "\n",
    "registered_ds_small = input_ds_small.register(ws, ds_name_small, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "Now that you have created your dataset, you are ready to move to the [02_Training_Pipeline.ipynb](https://github.com/microsoft/solution-accelerator-many-models/blob/master/02_Training/02_Training_Pipeline.ipynb) notebook to train the models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:latest_sdk] *",
   "language": "python",
   "name": "conda-env-latest_sdk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

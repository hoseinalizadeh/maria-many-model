{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add in overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites \n",
    "\n",
    "This example runs on an Azure Machine Learning Notebook VM. We are calling models that have **already been trained and registered** to the Workspace. If you have already run the Environment Setup and Training Pipeline notebooks or you have an AML Notebook set up with Models registered to the Workspace you are all set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Workspace, Datastore, and Compute\n",
    "\n",
    "As we did in the Training Pipeline notebook, we need to call the Workspace. We also want to create variables for the datastore and compute cluster. \n",
    "\n",
    "### Connect to the workspace\n",
    "Create a workspace object. Workspace.from_config() reads the file config.json and loads the details into an object named ws. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach existing compute resource\n",
    "From the Environment Setup Notebook, we created a compute cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "compute = AmlCompute(ws, 'many-models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Datastore containing the Orange Juice sales data\n",
    "From the Generate Data Notebook, we uploaded the csv's for each Store and Brand comination. Use the .get_default_datastore() to save the datastore we uploaded the files into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Registerd FileDataset\n",
    "In the Data_Prep notebook, we uploaded our data to Blob storage then registered the folder of data as a FileDataset to the Workspace. We are call that Dataset in order to pass it as an input into our ParallelRunStep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell reads in subset and registered data from the dataprep notebook \n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "ds_name = '10modelsfiledataset'\n",
    "oj_ds = Dataset.get_by_name(ws, name = ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset the number of models to run\n",
    "If you would like to test the scoring pipeline with a subset of models use the following code to take a sample of the full dataset. To make sure the same subset is selected across the Training, Scoring and Forecasting Notebooks use the same seed value. Otherwise, run the commented out cell to run all 11,000+ models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oj_ds_subset = oj_ds.take_sample(0.01, seed = 1248)\n",
    "\n",
    "oj_input_data = oj_ds_subset.as_named_input('oj_series')\n",
    "\n",
    "# This line will create a dataset for the full 11,973 models. \n",
    "#oj_input_data = oj_ds.as_named_input('oj_series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry script for Forecasting\n",
    "To use the models to make forecasts, you need an **entry script** and a list of **dependencies**:\n",
    "\n",
    "#### An entry script\n",
    "This script accepts requests, scores the requests by using the model, and returns the results.\n",
    "- __init()__ - Typically this function loads the model into a global object. This function is run only once at the start of batch processing per worker node/process. init method can make use of following environment variables (ParallelRunStep input):\n",
    "    1.\tAZUREML_BI_OUTPUT_PATH â€“ output folder path\n",
    "- __run(mini_batch)__ - The method to be parallelized. Each invocation will have one minibatch.<BR>\n",
    "__mini_batch__: Batch inference will invoke run method and pass either a list or Pandas DataFrame as an argument to the method. Each entry in mini_batch will be - a filepath if input is a FileDataset, a Pandas DataFrame if input is a TabularDataset.<BR>\n",
    "__run__ method response: run() method should return a Pandas DataFrame or an array. For append_row output_action, these returned elements are appended into the common output file. For summary_only, the contents of the elements are ignored. For all output actions, each returned output element indicates one successful inference of input element in the input mini-batch.\n",
    "    User should make sure that enough data is included in inference result to map input to inference. Inference output will be written in output file and not guaranteed to be in order, user should use some key in the output to map it to input.\n",
    "    \n",
    "\n",
    "#### Dependencies\n",
    "Helper scripts or Python/Conda packages required to run the entry script or model.\n",
    "\n",
    "The deployment configuration for the compute target that hosts the deployed model. This configuration describes things like memory and CPU requirements needed to run the model.\n",
    "\n",
    "These items are encapsulated into an inference configuration and a deployment configuration. The inference configuration references the entry script and other dependencies. You define these configurations programmatically when you use the SDK to perform the deployment. You define them in JSON files when you use the CLI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Run the batch inferece pipeline\n",
    "Now that the data, models, and compute resources are set up, we can put together a pipeline for forecasting. \n",
    "### Set up the environment to run the script\n",
    "Specify the conda dependencies for your script. This will allow us to install packages and configure the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the configuration to wrap the inference script \n",
    "In the [ParallelRunConfig](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunconfig?view=azure-ml-py), you will want to determine the number of workers and nodes appropriate for your use case. \n",
    "The _workercount_ is based off the number of cores on the VM. \n",
    "The _nodecount_ will determine the number of nodes to use. Increasing the node count should help to speed up the process. \n",
    "\n",
    "**include info here about runs we tried**\n",
    "\n",
    "You should set the _timeout_ to be the slightly longer than amount of time it would take for one iteration of your script to complete. In this example, that would be the amount of time to pull down a model and make predictions. The default time is 60 seconds. \n",
    "\n",
    "We added tags for additional information about our settings for the step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig \n",
    "\n",
    "workercount = 5\n",
    "nodecount = 8\n",
    "timeout = 3000\n",
    "\n",
    "compute = AmlCompute(ws, \"train-max\")\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'summary_only', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ParallelRunStep\n",
    "This is where we will call the entry script, environment configuration, and parameters. This [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) is the main step in our pipeline.  \n",
    "\n",
    "#### We can specify the following parameters: \n",
    "- _input_ : We will provide the data that will be used in the entry_script. \n",
    "- _output_ : This is the directory where the output of the step will be written. \n",
    "- _models_ : This provides additional metadata about the models used in the step. \n",
    "- _arguments_ : You can specify arguments that you want to pass to the entry_script with this argument.\n",
    "\n",
    "_arguments_ and _inputs_ are the two parameters that can pass information to the entry_script. \n",
    "\n",
    "For the orange juice sales forecasting, we have two arguments passed to the entry_script. *n_test_set* is the length of the test set which is also the number of predictions you would like to make. *timestamp_column* is the date column from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetname = 'store'\n",
    "output_dir = PipelineData(name = 'forecasting_output', \n",
    "                         datastore = dstore, \n",
    "                         output_path_on_compute = 'forecasting_output/')\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-scoring\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[oj_input_data],  \n",
    "    output=output_dir,\n",
    "    models= [], # this is just for logging\n",
    "    arguments=['--forecast_horizon', 8,\n",
    "              '--starting_date', '1992-10-01'],\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./scripts/forecasting.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "import pickle\n",
    "import logging \n",
    "\n",
    "# Import the AzureML packages \n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# Import the helper script \n",
    "from entry_script_helper import EntryScriptHelper\n",
    "\n",
    "\n",
    "# Get the information for the current Run\n",
    "thisrun = Run.get_context()\n",
    "\n",
    "# Set the log file name\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "# Parse the arguments passed in the PipelineStep through the arguments option \n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--forecast_horizon\", type=int, help=\"input number of predictions\")\n",
    "parser.add_argument(\"--starting_date\", type=str, help=\"date to begin forcasting\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(\"Argument 1(forecast_horizon): %s\" % args.forecast_horizon)\n",
    "print(\"Argument 2(starting_date): %s\" % args.starting_date)\n",
    "\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")    \n",
    "    return\n",
    "\n",
    "def run(input_data):\n",
    "    print(\"begin run \")\n",
    "    \n",
    "    # 0. Set up Logging\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    resultsList = []\n",
    "    #predictions = pd.DataFrame()\n",
    "    logger.info('making forecasts...')\n",
    "    \n",
    "    print('looping through data')\n",
    "    # 1. Loop through the input data \n",
    "    for idx, file in enumerate(input_data): # add the enumerate for the 12,000 files \n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]        \n",
    "        logs = []\n",
    "\n",
    "        date1=datetime.datetime.now()\n",
    "        logger.info('starting ('+file+') ' + str(date1))\n",
    "        thisrun.log(mname,'starttime-'+str(date1))\n",
    "\n",
    "        # 2. Set up data to predict on \n",
    "        store = [str(file).split('/')[-1][:-4].split('_')[0]] * args.forecast_horizon\n",
    "        brand = [split('/')[-1][:-4].split('_')[-1]] * args.forecast_horizon\n",
    "        date_list = pd.date_range(args.starting_date, periods = args.forecast_horizon, freq ='W-THU')\n",
    "        \n",
    "        prediciton_df = pd.DataFrame(list(zip(date_list, store, brand)), \n",
    "                                    columns = ['WeekStarting', 'Store', 'Brand'])\n",
    "        \n",
    "        # 3. Unpickle Model and Make Predictions             \n",
    "        model_name = 'arima_'+str(file).split('/')[-1][:-4]  \n",
    "        model_path = Model.get_model_path(model_name)         \n",
    "        model = joblib.load(model_path)        \n",
    "        \n",
    "        prediction_list, conf_int = model.predict(args.forecast_horizon, return_conf_int = True)\n",
    "\n",
    "        prediction_df['Predictions'] = prediction_list\n",
    "\n",
    "        # 4. Save the output back to blob storage \n",
    "        run_date = datetime.datetime.now().date()\n",
    "        ws1 = thisrun.experiment.workspace\n",
    "        output_path = os.path.join('./outputs/', model_name + str(run_date))\n",
    "        test.to_csv(path_or_buf=output_path + '.csv', index = False)\n",
    "        dstore = ws1.get_default_datastore()\n",
    "        dstore.upload_files([output_path + '.csv'], target_path='oj_forecasts' + str(run_date), overwrite=False, show_progress=True)\n",
    "\n",
    "        # 5. Append the predictions to return a dataframe if desired \n",
    "\n",
    "        # 6. Log Metrics\n",
    "        date2=datetime.datetime.now()\n",
    "        logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "        logs.append(str(file).split('/')[-1][:-4])\n",
    "        logs.append(model_name)\n",
    "        logs.append(str(date1))\n",
    "        logs.append(str(date2))\n",
    "        logs.append(date2-date1)\n",
    "        logs.append(idx)\n",
    "        logs.append(len(input_data))\n",
    "        logs.append(thisrun.get_status())        \n",
    "\n",
    "        thisrun.log(mname,'endtime-'+str(date2))\n",
    "        thisrun.log(mname,'auc-1')\n",
    "\n",
    "    resultsList.append(logs)\n",
    "    return resultsList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

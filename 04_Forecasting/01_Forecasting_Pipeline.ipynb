{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add in overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites \n",
    "\n",
    "This example runs on an Azure Machine Learning Notebook VM. We are calling models that have already been trained and registered to the Workspace. If you have already run the Environment Setup and Training Pipeline notebooks or you have an AML Notebook set up with Models registered to the Workspace you are all set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "compute = AmlCompute(ws, 'many-models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "ds_name = 'oj_data_100100100' # this FDS has 2,000 files in it\n",
    "\n",
    "stores_FDS = Dataset.get_by_name(ws, name=ds_name)\n",
    "\n",
    "# subset the data with .take_sample()\n",
    "\n",
    "stores_FDS_subset = stores_FDS.take_sample(0.01) # set the proportion of data you want to use \n",
    "\n",
    "stores_input = stores_FDS_subset.as_named_input('subset_stores') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig \n",
    "\n",
    "workercount = 5\n",
    "nodecount = 8\n",
    "timeout = 3000\n",
    "\n",
    "compute = AmlCompute(ws, \"train-max\")\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'summary_only', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig \n",
    "\n",
    "workercount = 5\n",
    "nodecount = 8\n",
    "timeout = 3000\n",
    "\n",
    "compute = AmlCompute(ws, \"train-max\")\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'forecasting.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'summary_only', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the inputs are set up for running 3 models currently. \n",
    "datasetname = 'store'\n",
    "output_dir = PipelineData(name = 'scoringOutput', \n",
    "                         datastore = dstore, \n",
    "                         output_path_on_compute = 'scoringOutput/')\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-scoring\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[stores_input],  \n",
    "    output=output_dir,\n",
    "    models= [], # this is just for logging\n",
    "    arguments=['--forecast_horizon', 8,\n",
    "              '--starting_date', '1992-10-01'],\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./scripts/forecasting.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from joblib import dump, load\n",
    "import pmdarima as pm\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "import pickle\n",
    "import logging \n",
    "\n",
    "# Import the AzureML packages \n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Experiment, Workspace, Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# Import the helper script \n",
    "from entry_script_helper import EntryScriptHelper\n",
    "\n",
    "\n",
    "# Get the information for the current Run\n",
    "thisrun = Run.get_context()\n",
    "\n",
    "# Set the log file name\n",
    "LOG_NAME = \"user_log\"\n",
    "\n",
    "# Parse the arguments passed in the PipelineStep through the arguments option \n",
    "parser = argparse.ArgumentParser(\"split\")\n",
    "parser.add_argument(\"--forecast_horizon\", type=int, help=\"input number of predictions\")\n",
    "parser.add_argument(\"--starting_date\", type=str, help=\"date to begin forcasting\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print(\"Argument 1(forecast_horizon): %s\" % args.forecast_horizon)\n",
    "print(\"Argument 2(starting_date): %s\" % args.starting_date)\n",
    "\n",
    "\n",
    "def init():\n",
    "    EntryScriptHelper().config(LOG_NAME)\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    output_folder = os.path.join(os.environ.get(\"AZ_BATCHAI_INPUT_AZUREML\", \"\"), \"temp/output\")\n",
    "    logger.info(f\"{__file__}.output_folder:{output_folder}\")\n",
    "    logger.info(\"init()\")    \n",
    "    return\n",
    "\n",
    "def run(input_data):\n",
    "    print(\"begin run \")\n",
    "    \n",
    "    # 0. Set up Logging\n",
    "    logger = logging.getLogger(LOG_NAME)\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "    resultsList = []\n",
    "    predictions = pd.DataFrame()\n",
    "    logger.info('making predictions...')\n",
    "    \n",
    "    print('looping through data')\n",
    "    # 1. Loop through the input data \n",
    "    for idx, file in enumerate(input_data): # add the enumerate for the 12,000 files \n",
    "        u1 = uuid.uuid4()\n",
    "        mname='arima'+str(u1)[0:16]        \n",
    "        logs = []\n",
    "\n",
    "        date1=datetime.datetime.now()\n",
    "        logger.info('starting ('+file+') ' + str(date1))\n",
    "        thisrun.log(mname,'starttime-'+str(date1))\n",
    "\n",
    "        # 2. Set up data to predict on \n",
    "        store = [str(file).split('/')[-1][:-4].split('_')[0]] * args.forecast_horizon\n",
    "        brand = [split('/')[-1][:-4].split('_')[-1]] * args.forecast_horizon\n",
    "        date_list = pd.date_range(args.starting_date, periods = args.forecast_horizon, freq ='W-THU')\n",
    "        \n",
    "        prediciton_df = pd.DataFrame(list(zip(date_list, store, brand)), \n",
    "                                    columns = ['WeekStarting', 'Store', 'Brand'])\n",
    "        \n",
    "        # 3. Unpickle Model and Make Predictions             \n",
    "        model_name = 'arima_'+str(file).split('/')[-1][:-4]  \n",
    "        model_path = Model.get_model_path(model_name)         \n",
    "        model = joblib.load(model_path)        \n",
    "        \n",
    "        prediction_list, conf_int = model.predict(args.forecast_horizon, return_conf_int = True)\n",
    "\n",
    "        prediction_df['Predictions'] = prediction_list\n",
    "\n",
    "        # 4. Save the output back to blob storage \n",
    "        run_date = datetime.datetime.now().date()\n",
    "        ws1 = thisrun.experiment.workspace\n",
    "        output_path = os.path.join('./outputs/', model_name + str(run_date))\n",
    "        test.to_csv(path_or_buf=output_path + '.csv', index = False)\n",
    "        dstore = ws1.get_default_datastore()\n",
    "        dstore.upload_files([output_path + '.csv'], target_path='oj_forecasts' + str(run_date), overwrite=False, show_progress=True)\n",
    "\n",
    "        # 5. Append the predictions to return a dataframe if desired \n",
    "\n",
    "        # 6. Log Metrics\n",
    "        date2=datetime.datetime.now()\n",
    "        logger.info('ending ('+str(file)+') ' + str(date2))\n",
    "\n",
    "        logs.append(str(file).split('/')[-1][:-4])\n",
    "        logs.append(model_name)\n",
    "        logs.append(str(date1))\n",
    "        logs.append(str(date2))\n",
    "        logs.append(date2-date1)\n",
    "        logs.append(idx)\n",
    "        logs.append(len(input_data))\n",
    "        logs.append(thisrun.get_status())        \n",
    "\n",
    "        thisrun.log(mname,'endtime-'+str(date2))\n",
    "        thisrun.log(mname,'auc-1')\n",
    "\n",
    "    resultsList.append(logs)\n",
    "    return resultsList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

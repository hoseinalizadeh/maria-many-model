{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Forecasting_Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we create a pipeline for Forcasting 11,000+ ARIMA models. The training and scoring of these models was completed in the Training and Scoring notebooks in this repository. We will set up the Pipeline for forecasting given the desired forecasting horizon. We utitlize the [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) to parallelize the process. For more information about the Data and Models refer to the Data Preparation and Training Notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "This example runs on an Azure Machine Learning Notebook VM. We are calling models that have **already been trained and registered** to the Workspace. If you have already run the Environment Setup and Training Pipeline notebooks or you have an AML Notebook set up with Models registered to the Workspace you are all set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Call the Workspace, Datastore, and Compute\n",
    "\n",
    "As we did in the Training Pipeline notebook, we need to call the Workspace. We also want to create variables for the datastore and compute cluster. \n",
    "\n",
    "### Connect to the workspace\n",
    "Create a workspace object. *Workspace.from_config()* reads the config.json file and loads the details into an object named ws. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach existing compute resource\n",
    "From the Environment Setup Notebook, we created a compute cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "compute = AmlCompute(ws, 'train-max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Datastore containing the Orange Juice sales data\n",
    "From the Data Preparation Notebook, we uploaded the csv's for each Store and Brand combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Call Registered FileDataset\n",
    "In the Data Preparation notebook, we uploaded our data to Blob storage then registered the folder of data as a FileDataset to the Workspace. You can choose to run the pipeline on the subet of 10 series or the full dataset of 11,000+ series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "FileDst10Models = Dataset.get_by_name(ws, name='oj_data_small')\n",
    "FileDst10ModelsInput = FileDst10Models.as_named_input('Forecast10Models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the block below to forecast for all 11,000+ series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FileDstAllModels = Dataset.get_by_name(ws, name='oj_data')\n",
    "#FileDstAllModelsInputs = FileDstAllModels.as_named_input('ForecastAllmodels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Build and Run the batch inferece pipeline\n",
    "Now that the data, models, and compute resources are set up, we can put together a pipeline for forecasting. \n",
    "### Set up the environment to run the script\n",
    "Specify the conda dependencies for your script. This will allow us to install packages and configure the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# set up the batch environment settings\n",
    "batch_conda_deps = CondaDependencies.create(pip_packages=['sklearn','pmdarima'])\n",
    "\n",
    "batch_env = Environment(name=\"manymodels_environment\")\n",
    "batch_env.python.conda_dependencies = batch_conda_deps\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the configuration to wrap the inference script \n",
    "In the [ParallelRunConfig](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallelrunconfig?view=azure-ml-py), we will call the entry script, environment configuration, and parameters. You will want to determine the number of workers and nodes appropriate for your use case.\n",
    "- The **workercount** is based off the number of cores on the VM. The compute cluster we set up has 8 cores therefore we set our worker count to 8.  \n",
    "- The **nodecount** will determine the number of nodes to use. Increasing the node count should help to speed up the process. We started with 3 then increased the number in order for the job to complete in under an hour. \n",
    "- You should set the **timeout** to be the slightly longer than amount of time it would take for one iteration of your script to complete. In this example, that would be the amount of time to pull down a model and make predictions. The default time is 60 seconds. \n",
    "\n",
    "We added tags for additional information about our settings for the step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig \n",
    "\n",
    "workercount = 5\n",
    "nodecount = 8\n",
    "timeout = 500\n",
    "\n",
    "tags1 = {}\n",
    "tags1['nodes'] = nodecount\n",
    "tags1['workers-per-node'] = workercount\n",
    "tags1['timeout'] = timeout \n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory = './scripts',\n",
    "    entry_script = 'score.py',\n",
    "    mini_batch_size = '1',\n",
    "    run_invocation_timeout = timeout, \n",
    "    error_threshold = 10,\n",
    "    output_action = 'append_row', \n",
    "    environment = batch_env, \n",
    "    process_count_per_node = workercount, \n",
    "    compute_target = compute, \n",
    "    node_count = nodecount\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ParallelRunStep\n",
    " This [ParallelRunStep](https://docs.microsoft.com/en-us/python/api/azureml-contrib-pipeline-steps/azureml.contrib.pipeline.steps.parallel_run_step.parallelrunstep?view=azure-ml-py) is the main step in our pipeline. We specified the following parameters: **input**, **output**, and **arguments**. We also set the output directory.   \n",
    "\n",
    "For the orange juice sales forecasting, we have four arguments passed to the entry_script. \n",
    "- **forecast_horizon** is how far into the future the forecast should extend.\n",
    "- **starting_date** is the date to begin forecating. \n",
    "- **output_datastore** is the datastore on blob you would like the output to be written to. \n",
    "- **overwrite_forecasting** sets if the forecasts will be overwritten.\n",
    "\n",
    "*arguments* and *inputs* are the two parameters that can pass information to the entry_script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "datasetname = 'store'\n",
    "output_dir = PipelineData(name = 'forecasting_output', \n",
    "                         datastore = dstore)\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"many-models-forecasting\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[FileDst10ModelsInput],  \n",
    "    output=output_dir,\n",
    "    models= [], \n",
    "    arguments=['--forecast_horizon', 8,\n",
    "              '--starting_date', '1992-10-01',\n",
    "              '--output_datastore', 'forecasting_output_datastore',\n",
    "              '--overwrite_forecasting', True],\n",
    "    allow_reuse = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Output Step\n",
    "We create a second [PythonScriptStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.python_script_step.pythonscriptstep) in the pipeline to retrieve the output from the ParallelRunStep and upload the output to a specificed path in Blob storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up RunConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "conda_run_config.target = compute\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-pipeline-core'], conda_packages=['pandas'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PythonScriptStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "upload_predictions_python_script_step = PythonScriptStep(name=\"upload_predictions\",\n",
    "                        script_name=\"upload_predictions.py\",\n",
    "                        compute_target=compute,\n",
    "                        source_directory='./scripts',\n",
    "                        runconfig=conda_run_config,\n",
    "                        arguments=['--ParallelRunStep_name','many-models-forecasting', \n",
    "                                   '--pipeline_output_name', 'forecasting_output', \n",
    "                                   '--datastore', 'forecasting_output_datastore', \n",
    "                                   '--experiment', 'automl-ojforecasting', \n",
    "                                   '--overwrite_predictions', True],\n",
    "                        allow_reuse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up step sequence  \n",
    "We set up a step sequence to ensure the ParallelRunStep executes before the PythonScriptStep. We need to ParallelRunStep to fully complete to collect the logs from each run and output them to the specificed directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import StepSequence\n",
    "\n",
    "forecasting_steps = StepSequence(steps=[parallelrun_step, upload_predictions_python_script_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and Run the Pipeline\n",
    "Create an Experiment to track the runs of the pipeline. Then, you can run the pipeline and review the output. With the current settings and the Standard_D13_V2 VM the pipeline takes approximately 1h 5m to run forecasts for all 11,000+ models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the experiment\n",
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, 'forecasting-pipeline-AP')\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=forecasting_steps)\n",
    "\n",
    "run = experiment.submit(pipeline, tags=tags1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Publish the pipeline\n",
    "After successfully setting up the pipeline, we publish the pipeline to the Workspace. [Published pipelines](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.publishedpipeline?view=azure-ml-py) create an endpoint to call the pipeline to run without having to open the code used to create it. It can be used to resubmit the pipline with different parameter inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'forecast-all-models',\n",
    "                                     description = 'forecast 11,973 models and log the run',\n",
    "                                     version = '1',\n",
    "                                     continue_on_step_failure = True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Schdule the pipeline weekly\n",
    "We can use the pipeline id to scheudle the pipeline to run at a specified interval. We schedule our forecasting piplein to run weekly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "    \n",
    "forecast_pipeline_id = published_pipeline.id\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, start_time=\"2020-01-01T13:00:00\")\n",
    "\n",
    "recurring_schedule = Schedule.create(ws, name=\"Forecasting-Pipeline-Recurring-Schedule\", \n",
    "                            description=\"Schedule forecasting Pipeline to run on the first day of every month starting Jan 1, 2020 at 1PM\",\n",
    "                            pipeline_id=forecast_pipeline_id, \n",
    "                            experiment_name=experiment.name, \n",
    "                            recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Pipeline Outputs\n",
    "The forecasting pipeline forecasts the orange juice quantity for a Store by Brand for the next 8 weeks. The pipeline returns one file with the predictions for each store and outputs the result to the forecasting_output Blob container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
